<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Spark RDD简介及使用</title>
      <link href="/posts/1754.html"/>
      <url>/posts/1754.html</url>
      
        <content type="html"><![CDATA[<h1 id="弹性式数据集RDDs"><a href="#弹性式数据集RDDs" class="headerlink" title="弹性式数据集RDDs"></a>弹性式数据集RDDs</h1><nav><a href="#一RDD简介">一、RDD简介</a><br><a href="#二创建RDD">二、创建RDD</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#21-由现有集合创建">2.1 由现有集合创建</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#22-引用外部存储系统中的数据集">2.2 引用外部存储系统中的数据集</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#23-textFile--wholeTextFiles">2.3 textFile &amp; wholeTextFiles</a><br><a href="#三操作RDD">三、操作RDD</a><br><a href="#四缓存RDD">四、缓存RDD</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#41-缓存级别">4.1 缓存级别</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#42-使用缓存">4.2 使用缓存</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#43-移除缓存">4.3 移除缓存</a><br><a href="#五理解shuffle">五、理解shuffle</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#51-shuffle介绍">5.1 shuffle介绍</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#52-Shuffle的影响">5.2 Shuffle的影响</a><br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<a href="#53-导致Shuffle的操作">5.3 导致Shuffle的操作</a><br><a href="#五宽依赖和窄依赖">五、宽依赖和窄依赖</a><br><a href="#六DAG的生成">六、DAG的生成</a><br></nav><h2 id="一、RDD简介"><a href="#一、RDD简介" class="headerlink" title="一、RDD简介"></a>一、RDD简介</h2><p><code>RDD</code> 全称为 Resilient Distributed Datasets，是 Spark 最基本的数据抽象，它是只读的、分区记录的集合，支持并行操作，可以由外部数据集或其他 RDD 转换而来，它具有以下特性：</p><ul><li>一个 RDD 由一个或者多个分区（Partitions）组成。对于 RDD 来说，每个分区会被一个计算任务所处理，用户可以在创建 RDD 时指定其分区个数，如果没有指定，则默认采用程序所分配到的 CPU 的核心数；</li><li>RDD 拥有一个用于计算分区的函数 compute；</li><li>RDD 会保存彼此间的依赖关系，RDD 的每次转换都会生成一个新的依赖关系，这种 RDD 之间的依赖关系就像流水线一样。在部分分区数据丢失后，可以通过这种依赖关系重新计算丢失的分区数据，而不是对 RDD 的所有分区进行重新计算；</li><li>Key-Value 型的 RDD 还拥有 Partitioner(分区器)，用于决定数据被存储在哪个分区中，目前 Spark 中支持 HashPartitioner(按照哈希分区) 和 RangeParationer(按照范围进行分区)；</li><li>一个优先位置列表 (可选)，用于存储每个分区的优先位置 (prefered location)。对于一个 HDFS 文件来说，这个列表保存的就是每个分区所在的块的位置，按照“移动数据不如移动计算“的理念，Spark 在进行任务调度的时候，会尽可能的将计算任务分配到其所要处理数据块的存储位置。</li></ul><p><code>RDD[T]</code> 抽象类的部分相关代码如下：</p><pre class="line-numbers language-scala"><code class="language-scala"><span class="token comment" spellcheck="true">// 由子类实现以计算给定分区</span><span class="token keyword">def</span> compute<span class="token punctuation">(</span>split<span class="token operator">:</span> Partition<span class="token punctuation">,</span> context<span class="token operator">:</span> TaskContext<span class="token punctuation">)</span><span class="token operator">:</span> Iterator<span class="token punctuation">[</span>T<span class="token punctuation">]</span><span class="token comment" spellcheck="true">// 获取所有分区</span><span class="token keyword">protected</span> <span class="token keyword">def</span> getPartitions<span class="token operator">:</span> Array<span class="token punctuation">[</span>Partition<span class="token punctuation">]</span><span class="token comment" spellcheck="true">// 获取所有依赖关系</span><span class="token keyword">protected</span> <span class="token keyword">def</span> getDependencies<span class="token operator">:</span> Seq<span class="token punctuation">[</span>Dependency<span class="token punctuation">[</span>_<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> deps<span class="token comment" spellcheck="true">// 获取优先位置列表</span><span class="token keyword">protected</span> <span class="token keyword">def</span> getPreferredLocations<span class="token punctuation">(</span>split<span class="token operator">:</span> Partition<span class="token punctuation">)</span><span class="token operator">:</span> Seq<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> Nil<span class="token comment" spellcheck="true">// 分区器 由子类重写以指定它们的分区方式</span><span class="token annotation punctuation">@transient</span> <span class="token keyword">val</span> partitioner<span class="token operator">:</span> Option<span class="token punctuation">[</span>Partitioner<span class="token punctuation">]</span> <span class="token operator">=</span> None<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="二、创建RDD"><a href="#二、创建RDD" class="headerlink" title="二、创建RDD"></a>二、创建RDD</h2><p>RDD 有两种创建方式，分别介绍如下：</p><h3 id="2-1-由现有集合创建"><a href="#2-1-由现有集合创建" class="headerlink" title="2.1 由现有集合创建"></a>2.1 由现有集合创建</h3><p>这里使用 <code>spark-shell</code> 进行测试，启动命令如下：</p><pre class="line-numbers language-shell"><code class="language-shell">spark-shell --master local[4]<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>启动 <code>spark-shell</code> 后，程序会自动创建应用上下文，相当于执行了下面的 Scala 语句：</p><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">val</span> conf <span class="token operator">=</span> <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">"Spark shell"</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">"local[4]"</span><span class="token punctuation">)</span><span class="token keyword">val</span> sc <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>conf<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>由现有集合创建 RDD，你可以在创建时指定其分区个数，如果没有指定，则采用程序所分配到的 CPU 的核心数：</p><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">val</span> data <span class="token operator">=</span> Array<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">// 由现有集合创建 RDD,默认分区数为程序所分配到的 CPU 的核心数</span><span class="token keyword">val</span> dataRDD <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span>data<span class="token punctuation">)</span> <span class="token comment" spellcheck="true">// 查看分区数</span>dataRDD<span class="token punctuation">.</span>getNumPartitions<span class="token comment" spellcheck="true">// 明确指定分区数</span><span class="token keyword">val</span> dataRDD <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span>data<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>执行结果如下：</p><div align="center"> <img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="/posts/1754.htm/pictures/scala-分区数.png"> </div>### 2.2 引用外部存储系统中的数据集<p>引用外部存储系统中的数据集，例如本地文件系统，HDFS，HBase 或支持 Hadoop InputFormat 的任何数据源。</p><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">val</span> fileRDD <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">"/usr/file/emp.txt"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">// 获取第一行文本</span>fileRDD<span class="token punctuation">.</span>take<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>使用外部存储系统时需要注意以下两点：</p><ul><li>如果在集群环境下从本地文件系统读取数据，则要求该文件必须在集群中所有机器上都存在，且路径相同；</li><li>支持目录路径，支持压缩文件，支持使用通配符。</li></ul><h3 id="2-3-textFile-amp-wholeTextFiles"><a href="#2-3-textFile-amp-wholeTextFiles" class="headerlink" title="2.3 textFile &amp; wholeTextFiles"></a>2.3 textFile &amp; wholeTextFiles</h3><p>两者都可以用来读取外部文件，但是返回格式是不同的：</p><ul><li><strong>textFile</strong>：其返回格式是 <code>RDD[String]</code> ，返回的是就是文件内容，RDD 中每一个元素对应一行数据；</li><li><strong>wholeTextFiles</strong>：其返回格式是 <code>RDD[(String, String)]</code>，元组中第一个参数是文件路径，第二个参数是文件内容；</li><li>两者都提供第二个参数来控制最小分区数；</li><li>从 HDFS 上读取文件时，Spark 会为每个块创建一个分区。</li></ul><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">def</span> textFile<span class="token punctuation">(</span>path<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">,</span>minPartitions<span class="token operator">:</span> <span class="token builtin">Int</span> <span class="token operator">=</span> defaultMinPartitions<span class="token punctuation">)</span><span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> withScope <span class="token punctuation">{</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span><span class="token keyword">def</span> wholeTextFiles<span class="token punctuation">(</span>path<span class="token operator">:</span> <span class="token builtin">String</span><span class="token punctuation">,</span>minPartitions<span class="token operator">:</span> <span class="token builtin">Int</span> <span class="token operator">=</span> defaultMinPartitions<span class="token punctuation">)</span><span class="token operator">:</span> RDD<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">String</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token operator">=</span><span class="token punctuation">{</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="三、操作RDD"><a href="#三、操作RDD" class="headerlink" title="三、操作RDD"></a>三、操作RDD</h2><p>RDD 支持两种类型的操作：<em>transformations<em>（转换，从现有数据集创建新数据集）和 *actions</em>（在数据集上运行计算后将值返回到驱动程序）。RDD 中的所有转换操作都是惰性的，它们只是记住这些转换操作，但不会立即执行，只有遇到 *action</em> 操作后才会真正的进行计算，这类似于函数式编程中的惰性求值。</p><pre class="line-numbers language-scala"><code class="language-scala"><span class="token keyword">val</span> list <span class="token operator">=</span> List<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">// map 是一个 transformations 操作，而 foreach 是一个 actions 操作</span>sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span>list<span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span>_ <span class="token operator">*</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span><span class="token comment" spellcheck="true">// 输出： 10 20 30</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h2 id="四、缓存RDD"><a href="#四、缓存RDD" class="headerlink" title="四、缓存RDD"></a>四、缓存RDD</h2><h3 id="4-1-缓存级别"><a href="#4-1-缓存级别" class="headerlink" title="4.1 缓存级别"></a>4.1 缓存级别</h3><p>Spark 速度非常快的一个原因是 RDD 支持缓存。成功缓存后，如果之后的操作使用到了该数据集，则直接从缓存中获取。虽然缓存也有丢失的风险，但是由于 RDD 之间的依赖关系，如果某个分区的缓存数据丢失，只需要重新计算该分区即可。</p><p>Spark 支持多种缓存级别 ：</p><table><thead><tr><th>Storage Level<br>（存储级别）</th><th>Meaning（含义）</th></tr></thead><tbody><tr><td><code>MEMORY_ONLY</code></td><td>默认的缓存级别，将 RDD 以反序列化的 Java 对象的形式存储在 JVM 中。如果内存空间不够，则部分分区数据将不再缓存。</td></tr><tr><td><code>MEMORY_AND_DISK</code></td><td>将 RDD 以反序列化的 Java 对象的形式存储 JVM 中。如果内存空间不够，将未缓存的分区数据存储到磁盘，在需要使用这些分区时从磁盘读取。</td></tr><tr><td><code>MEMORY_ONLY_SER</code><br></td><td>将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式比反序列化对象节省存储空间，但在读取时会增加 CPU 的计算负担。仅支持 Java 和 Scala 。</td></tr><tr><td><code>MEMORY_AND_DISK_SER</code><br></td><td>类似于 <code>MEMORY_ONLY_SER</code>，但是溢出的分区数据会存储到磁盘，而不是在用到它们时重新计算。仅支持 Java 和 Scala。</td></tr><tr><td><code>DISK_ONLY</code></td><td>只在磁盘上缓存 RDD</td></tr><tr><td><code>MEMORY_ONLY_2</code>, <br><code>MEMORY_AND_DISK_2</code>, etc</td><td>与上面的对应级别功能相同，但是会为每个分区在集群中的两个节点上建立副本。</td></tr><tr><td><code>OFF_HEAP</code></td><td>与 <code>MEMORY_ONLY_SER</code> 类似，但将数据存储在堆外内存中。这需要启用堆外内存。</td></tr></tbody></table><blockquote><p>启动堆外内存需要配置两个参数：</p><ul><li><strong>spark.memory.offHeap.enabled</strong> ：是否开启堆外内存，默认值为 false，需要设置为 true；</li><li><strong>spark.memory.offHeap.size</strong> : 堆外内存空间的大小，默认值为 0，需要设置为正值。</li></ul></blockquote><h3 id="4-2-使用缓存"><a href="#4-2-使用缓存" class="headerlink" title="4.2 使用缓存"></a>4.2 使用缓存</h3><p>缓存数据的方法有两个：<code>persist</code> 和 <code>cache</code> 。<code>cache</code> 内部调用的也是 <code>persist</code>，它是 <code>persist</code> 的特殊化形式，等价于 <code>persist(StorageLevel.MEMORY_ONLY)</code>。示例如下：</p><pre class="line-numbers language-scala"><code class="language-scala"><span class="token comment" spellcheck="true">// 所有存储级别均定义在 StorageLevel 对象中</span>fileRDD<span class="token punctuation">.</span>persist<span class="token punctuation">(</span>StorageLevel<span class="token punctuation">.</span>MEMORY_AND_DISK<span class="token punctuation">)</span>fileRDD<span class="token punctuation">.</span>cache<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="4-3-移除缓存"><a href="#4-3-移除缓存" class="headerlink" title="4.3 移除缓存"></a>4.3 移除缓存</h3><p>Spark 会自动监视每个节点上的缓存使用情况，并按照最近最少使用（LRU）的规则删除旧数据分区。当然，你也可以使用 <code>RDD.unpersist()</code> 方法进行手动删除。</p><h2 id="五、理解shuffle"><a href="#五、理解shuffle" class="headerlink" title="五、理解shuffle"></a>五、理解shuffle</h2><h3 id="5-1-shuffle介绍"><a href="#5-1-shuffle介绍" class="headerlink" title="5.1 shuffle介绍"></a>5.1 shuffle介绍</h3><p>在 Spark 中，一个任务对应一个分区，通常不会跨分区操作数据。但如果遇到 <code>reduceByKey</code> 等操作，Spark 必须从所有分区读取数据，并查找所有键的所有值，然后汇总在一起以计算每个键的最终结果 ，这称为 <code>Shuffle</code>。</p><div align="center"> <img width="600px" src="/posts/1754.htm/pictures/spark-reducebykey.png"> </div><h3 id="5-2-Shuffle的影响"><a href="#5-2-Shuffle的影响" class="headerlink" title="5.2 Shuffle的影响"></a>5.2 Shuffle的影响</h3><p>Shuffle 是一项昂贵的操作，因为它通常会跨节点操作数据，这会涉及磁盘 I/O，网络 I/O，和数据序列化。某些 Shuffle 操作还会消耗大量的堆内存，因为它们使用堆内存来临时存储需要网络传输的数据。Shuffle 还会在磁盘上生成大量中间文件，从 Spark 1.3 开始，这些文件将被保留，直到相应的 RDD 不再使用并进行垃圾回收，这样做是为了避免在计算时重复创建 Shuffle 文件。如果应用程序长期保留对这些 RDD 的引用，则垃圾回收可能在很长一段时间后才会发生，这意味着长时间运行的 Spark 作业可能会占用大量磁盘空间，通常可以使用 <code>spark.local.dir</code> 参数来指定这些临时文件的存储目录。</p><h3 id="5-3-导致Shuffle的操作"><a href="#5-3-导致Shuffle的操作" class="headerlink" title="5.3 导致Shuffle的操作"></a>5.3 导致Shuffle的操作</h3><p>由于 Shuffle 操作对性能的影响比较大，所以需要特别注意使用，以下操作都会导致 Shuffle：</p><ul><li><strong>涉及到重新分区操作</strong>： 如 <code>repartition</code> 和 <code>coalesce</code>；</li><li><strong>所有涉及到 ByKey 的操作</strong>：如 <code>groupByKey</code> 和 <code>reduceByKey</code>，但 <code>countByKey</code> 除外；</li><li><strong>联结操作</strong>：如 <code>cogroup</code> 和 <code>join</code>。</li></ul><h2 id="五、宽依赖和窄依赖"><a href="#五、宽依赖和窄依赖" class="headerlink" title="五、宽依赖和窄依赖"></a>五、宽依赖和窄依赖</h2><p>RDD 和它的父 RDD(s) 之间的依赖关系分为两种不同的类型：</p><ul><li><strong>窄依赖 (narrow dependency)</strong>：父 RDDs 的一个分区最多被子 RDDs 一个分区所依赖；</li><li><strong>宽依赖 (wide dependency)</strong>：父 RDDs 的一个分区可以被子 RDDs 的多个子分区所依赖。</li></ul><p>如下图，每一个方框表示一个 RDD，带有颜色的矩形表示分区：</p><div align="center"> <img width="600px" src="/posts/1754.htm/pictures/spark-窄依赖和宽依赖.png"> </div><p>区分这两种依赖是非常有用的：</p><ul><li>首先，窄依赖允许在一个集群节点上以流水线的方式（pipeline）对父分区数据进行计算，例如先执行 map 操作，然后执行 filter 操作。而宽依赖则需要计算好所有父分区的数据，然后再在节点之间进行 Shuffle，这与 MapReduce 类似。</li><li>窄依赖能够更有效地进行数据恢复，因为只需重新对丢失分区的父分区进行计算，且不同节点之间可以并行计算；而对于宽依赖而言，如果数据丢失，则需要对所有父分区数据进行计算并再次 Shuffle。</li></ul><h2 id="六、DAG的生成"><a href="#六、DAG的生成" class="headerlink" title="六、DAG的生成"></a>六、DAG的生成</h2><p>RDD(s) 及其之间的依赖关系组成了 DAG(有向无环图)，DAG 定义了这些 RDD(s) 之间的 Lineage(血统) 关系，通过血统关系，如果一个 RDD 的部分或者全部计算结果丢失了，也可以重新进行计算。那么 Spark 是如何根据 DAG 来生成计算任务呢？主要是根据依赖关系的不同将 DAG 划分为不同的计算阶段 (Stage)：</p><ul><li>对于窄依赖，由于分区的依赖关系是确定的，其转换操作可以在同一个线程执行，所以可以划分到同一个执行阶段；</li><li>对于宽依赖，由于 Shuffle 的存在，只能在父 RDD(s) 被 Shuffle 处理完成后，才能开始接下来的计算，因此遇到宽依赖就需要重新划分阶段。</li></ul><div align="center"> <img width="600px" height="600px" src="/posts/1754.htm/pictures/spark-DAG.png"> </div><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li>张安站 . Spark 技术内幕：深入解析 Spark 内核架构设计与实现原理[M] . 机械工业出版社 . 2015-09-01</li><li><a href="https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-programming-guide" target="_blank" rel="noopener">RDD Programming Guide</a></li><li><a href="http://shiyanjun.cn/archives/744.html" target="_blank" rel="noopener">RDD：基于内存的集群计算容错抽象</a></li></ol><p><strong>持续更新中…，如果遇到问题欢迎联系我，在文章最后评论区【留言和讨论】，当然，欢迎点击文章最后的打赏按键，请博主一杯冰阔乐，笑～</strong></p><table>  <tbody><tr>    <td><img width="100" src="https://hualei.online/medias/reward/alipay.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/wechat.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/zan.png"></td>     </tr></tbody></table><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 技术学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> 互联网 </tag>
            
            <tag> Sprak </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kafka简介及其使用</title>
      <link href="/posts/3bb2.html"/>
      <url>/posts/3bb2.html</url>
      
        <content type="html"><![CDATA[<p><strong>您可能感兴趣的文章:</strong></p><p><a href="http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MzgwNTU2Mg%3D%3D%26mid%3D100000472%26idx%3D1%26sn%3D99353b901d1174c3edd4a9ebbe394975%26chksm%3D7d3d444d4a4acd5bf0017210f55ec394abda01d163674d540988ca94863a51411be951711553%23rd" target="_blank" rel="noopener">Apache-Kafka核心概念</a></p><p><a href="http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MzgwNTU2Mg%3D%3D%26mid%3D100000476%26idx%3D1%26sn%3D34b2127b1a09664087e3b2079844c2db%26chksm%3D7d3d44494a4acd5f3bc70d914ae2842409282780d19d57043d168895e55f160b3be7835e2446%23rd" target="_blank" rel="noopener">Apache-Kafka核心组件和流程-协调器</a></p><p><a href="http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MzgwNTU2Mg%3D%3D%26mid%3D100000480%26idx%3D1%26sn%3D054cdf620eb82c4ecfaccd226d49d0e0%26chksm%3D7d3d44754a4acd638ca37afcfdaad802bb3dec01758b18cdf2c607ec494526832ee58ff43451%23rd" target="_blank" rel="noopener">Apache-Kafka核心组件和流程(副本管理器)</a></p><p><a href="http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MzgwNTU2Mg%3D%3D%26mid%3D100000474%26idx%3D1%26sn%3Dc9b9d8fbb942f5299eb1d23a9363c0a4%26chksm%3D7d3d444f4a4acd597607e33ee59aad92db50084a5ab7edb84449df6f2f3ecc504e97f05977bb%23rd" target="_blank" rel="noopener">Apache-Kafka 核心组件和流程-控制器</a></p><p><a href="http://link.zhihu.com/?target=http%3A//mp.weixin.qq.com/s%3F__biz%3DMzU3MzgwNTU2Mg%3D%3D%26mid%3D100000478%26idx%3D1%26sn%3Deeb3310214d7fa24ca86c4afad421baa%26chksm%3D7d3d444b4a4acd5d1987dc78f89d40a20833cec682b30b9f1a0735a26681f681a38853a6ff63%23rd" target="_blank" rel="noopener">Apache-Kafka核心组件和流程-日志管理器</a></p><p>….</p><h3 id="kafka的定位"><a href="#kafka的定位" class="headerlink" title="kafka的定位"></a><strong>kafka的定位</strong></h3><p>提到kafka，不太熟悉或者稍有接触的开发人员，第一想法可能会觉得它是一个消息系统。其实Kafka的定位并不止于此。</p><p>Kafka官方文档介绍说，Apache Kafka是一个分布式流平台，并给出了如下解释：</p><p>流平台有三个关键的能力：</p><ul><li>发布订阅记录流，和消息队列或者企业新消息系统类似。</li><li>以可容错、持久的方式保存记录流</li><li>当记录流产生时就进行处理</li></ul><p>Kafka通常用于应用中的两种广播类型：</p><ul><li>在系统和应用间建立实时的数据管道，能够可信赖的获取数据。</li><li>建立实时的流应用，可以处理或者响应数据流。</li></ul><p>由此可见，kafka给自身的定位并不只是一个消息系统，而是通过发布订阅消息这种机制实现了流平台。</p><p>其实不管kafka给自己的定位如何，他都逃脱不了发布订阅消息的底层机制。本文讲解的重点，也是kafka发布订阅消息的特性。</p><p>Kafka和大多数消息系统一样，搭建好kafka集群后，生产者向特定的topic生产消息，而消费者通过订阅topic，能够准实时的拉取到该topic新消息，进行消费。如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="http://upload-images.jianshu.io/upload_images/16241060-eabf90da50c94506.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><h3 id="Kafka特性"><a href="#Kafka特性" class="headerlink" title="Kafka特性"></a><strong>Kafka特性</strong></h3><p>kafka和有以下主要的特性：</p><ul><li>消息持久化</li><li>高吞吐量</li><li>可扩展性</li></ul><p>尤其是高吞吐量，是他的最大卖点。kafka之所以能够实现高吞吐量，是基于他自身优良的设计，及集群的可扩展性。后面章节会展开来分析。</p><h3 id="Kafka应用场景"><a href="#Kafka应用场景" class="headerlink" title="Kafka应用场景"></a><strong>Kafka应用场景</strong></h3><ul><li>消息系统</li><li>日志系统</li><li>流处理</li></ul><h3 id="单机环境"><a href="#单机环境" class="headerlink" title="单机环境"></a><strong>单机环境</strong></h3><p>官方建议使用JDK 1.8版本，因此本文使用的环境都是JDK1.8。关于JDK的安装，本文不再详述，默认Java环境已经具备。</p><p>由于Kafka依赖zookeeper，kafka通过zookeeper现实分布式系统的协调，所以我们需要先安装zookeeper。</p><p>接下来我们按照如下步骤，一步步来安装kafka：</p><p>1、下载zookeeper，解压。</p><p>下载地址：<a href="http://link.zhihu.com/?target=https%3A//zookeeper.apache.org/releases.html%23download" target="_blank" rel="noopener">https://zookeeper.apache.org/releases.html#download</a></p><p>2、创建zookeeper配置文件</p><p>在zookeeper解压后的目录下找到conf文件夹，进入后，复制文件zoo_sample.cfg，并命名为zoo.cfg</p><p>zoo.cfg中一共四个配置项，可以使用默认配置。</p><p>3、启动zookeeper。</p><p>进入zookeeper根目录执行 bin/zkServer.sh start</p><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="http://upload-images.jianshu.io/upload_images/16241060-45642ac6542526af.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p>4、下载kafka，解压。</p><p>kafka 2.0版本下载地址：<a href="http://link.zhihu.com/?target=https%3A//www.apache.org/dyn/closer.cgi%3Fpath%3D/kafka/2.0.0/kafka_2.11-2.0.0.tgz" target="_blank" rel="noopener">https://www.apache.org/dyn/closer.cgi?path=/kafka/2.0.0/kafka_2.11-2.0.0.tgz</a></p><p>5、修改kafka的配置文件</p><p>进入kafka根目录下的config文件夹下，打开server.properties,修改如下配置</p><p>zookeeper.connect=localhost:2181</p><p>broker.id=0</p><p>log.dirs=/tmp/kafka-logs</p><p>zookeeper.connect是zookeeper的链接信息，broker.id是当前kafka实例的id，log.dirs是kafka存储消息内容的路径。</p><p>6、启动kafka</p><p>进入kafka根目录执行 bin/kafka-server-start.sh config/server.properties</p><p>此命令告诉kaka启动时使用config/server.properties配置项</p><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="http://upload-images.jianshu.io/upload_images/16241060-7daff2291823ea47.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p>启动kafka后，如果控制台没有报错信息，那么kafka应该已经启动成功了，我们可以通过查看zookeeper中相关节点值来确认。步骤如下：</p><p>1、启动zookeeper的client</p><p>进入zookeeper根目录下，执行 bin/zkCli.sh -server 127.0.0.1:2181。启动成功后如下图</p><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="http://upload-images.jianshu.io/upload_images/16241060-e93e674f8687f166.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p>2、输入命令 ls /brokers，回车，可以看到如下信息：</p><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="http://upload-images.jianshu.io/upload_images/16241060-332ed3a117128b6a.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p>这些子节点存储的就是kafka集群管理的数据。broker是kafka的一个服务单元实例</p><p>3、我看看一下ids这个节点下的数据，输入命令 ls /brokers/ids，可以看到如下信息：</p><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="http://upload-images.jianshu.io/upload_images/16241060-d562b8ccbc167ca9.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p>还记得我们在配置单机环境时，修改的kafka配置项broker.id=0 吗？这里的0就是表示那个kafka的实例已经加入了kafka集群。</p><h3 id="集群环境"><a href="#集群环境" class="headerlink" title="集群环境"></a><strong>集群环境</strong></h3><p>集群环境的搭建也很简单，在单机环境的基础上，让多个单机连接到同一个zookeeper即可。需要注意两点：</p><p>1、每个实例设置不同的broker.id。</p><p>2、如果多个实例部署在同一台服务器，还要注意修改log.dirs为不同目录，确保消息存储时不会有冲突。集群环境的具体搭建，在此精简教程中不再做详细讨论。</p><p>发出你的第一条kafka消息</p><p>我们通过kafka带的工具来创建一个topic，然后尝试发送和消费一个消息，直观的去感受下kafka。</p><p>1、创建topic</p><p>进入kafka根目录，执行如下命令：</p><pre><code>bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic study</code></pre><p>执行成功后，创建了study这个topic，如下图所示：</p><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="http://upload-images.jianshu.io/upload_images/16241060-1ee1e88312b5607e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p>此命令行有几个参数，分别指明了zookeeper的链接信息，分区和副本的数量等。关于分区和副本后续会仔细讲解，现在不用过多关注。</p><p>2、启动消费者</p><p>我们开启一个消费者并且订阅study这个topic，执行如下命令:</p><pre><code>bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic study --from-beginning</code></pre><p>看到如下图，光标停留在最前面，没有任何信息输出，说明启动消费者成功，此时在等待新的消息。</p><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="http://upload-images.jianshu.io/upload_images/16241060-4c9ec1250f2a7d52.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p>3、开启生产者</p><p>新打开一个命令窗口，输入命令</p><pre><code>bin/kafka-console-producer.sh --broker-list localhost:9092 --topic study</code></pre><p>启动成功后，如下图，等待你输入新的消息。</p><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="http://upload-images.jianshu.io/upload_images/16241060-1dc35a39b6a6c1a6.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p>4、发送你的第一条消息</p><p>在上面生产者的窗口输入一条消息 hello kafka,点击回车，如下图：</p><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="http://upload-images.jianshu.io/upload_images/16241060-aa03f34cc9fdc77d.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p>此时切换到消费者的窗口，可以看到消费者已经消费到这条消息，在窗口中打印了出来。</p><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="http://upload-images.jianshu.io/upload_images/16241060-148e5147382d1712.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image"></p><p>至此我们走完了一个发送消息的流程，可以看到我们经历了创建topic、启动生产者、消费者、生产者生产消息、消费者消费消息，这几个步骤。</p><p>小结：通过本章节学习，相信你已经能够成功搭建起kafka单机环境，甚至集群环境。然后通过kafka自带的工具，直观的感受了kafka运转的整个过程。接下来的章节我们将会进入kafka的核心领域，也是本教程的重点章节，只有理解了kafka内在的设计理念和原理，才能做到活学活用。</p><p><strong>持续更新中…，如果遇到问题欢迎联系我，在文章最后评论区【留言和讨论】，当然，欢迎点击文章最后的打赏按键，请博主一杯冰阔乐，笑～</strong></p><table>  <tbody><tr>    <td><img width="100" src="https://hualei.online/medias/reward/alipay.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/wechat.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/zan.png"></td>     </tr></tbody></table><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 技术学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> 互联网 </tag>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM内存结构</title>
      <link href="/posts/6c2c.html"/>
      <url>/posts/6c2c.html</url>
      
        <content type="html"><![CDATA[<h1 id="JVM-内存结构"><a href="#JVM-内存结构" class="headerlink" title="JVM 内存结构"></a>JVM 内存结构</h1><p>Java 虚拟机的内存空间分为 5 个部分：</p><ul><li>程序计数器</li><li>Java 虚拟机栈</li><li>本地方法栈</li><li>堆</li><li>方法区<br><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/JVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84/%E5%9B%BE%E7%89%87%E4%B8%80.jpg" alt="图片一"><br>JDK 1.8 同 JDK 1.7 比，最大的差别就是：元数据区取代了永久代。元空间的本质和永久代类似，都是对 JVM 规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元数据空间并不在虚拟机中，而是使用本地内存。</li></ul><h2 id="程序计数器（PC-寄存器）"><a href="#程序计数器（PC-寄存器）" class="headerlink" title="程序计数器（PC 寄存器）"></a>程序计数器（PC 寄存器）</h2><h3 id="程序计数器的定义"><a href="#程序计数器的定义" class="headerlink" title="程序计数器的定义"></a>程序计数器的定义</h3><p>程序计数器是一块较小的内存空间，是当前线程正在执行的那条字节码指令的地址。若当前线程正在执行的是一个本地方法，那么此时程序计数器为<code>Undefined</code>。</p><h3 id="程序计数器的作用"><a href="#程序计数器的作用" class="headerlink" title="程序计数器的作用"></a>程序计数器的作用</h3><ul><li>字节码解释器通过改变程序计数器来依次读取指令，从而实现代码的流程控制。</li><li>在多线程情况下，程序计数器记录的是当前线程执行的位置，从而当线程切换回来时，就知道上次线程执行到哪了。</li></ul><h3 id="程序计数器的特点"><a href="#程序计数器的特点" class="headerlink" title="程序计数器的特点"></a>程序计数器的特点</h3><ul><li>是一块较小的内存空间。</li><li>线程私有，每条线程都有自己的程序计数器。</li><li>生命周期：随着线程的创建而创建，随着线程的结束而销毁。</li><li>是唯一一个不会出现<code>OutOfMemoryError</code>的内存区域。</li></ul><h2 id="Java-虚拟机栈（Java-栈）"><a href="#Java-虚拟机栈（Java-栈）" class="headerlink" title="Java 虚拟机栈（Java 栈）"></a>Java 虚拟机栈（Java 栈）</h2><h3 id="Java-虚拟机栈的定义"><a href="#Java-虚拟机栈的定义" class="headerlink" title="Java 虚拟机栈的定义"></a>Java 虚拟机栈的定义</h3><p>Java 虚拟机栈是描述 Java 方法运行过程的内存模型。</p><p>Java 虚拟机栈会为每一个即将运行的 Java 方法创建一块叫做“栈帧”的区域，用于存放该方法运行过程中的一些信息，如：</p><ul><li>局部变量表</li><li>操作数栈</li><li>动态链接</li><li>方法出口信息</li><li>……</li></ul><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/JVM%E5%86%85%E5%AD%98%E7%BB%93%E6%9E%84/%E5%9B%BE%E7%89%87%E4%BA%8C.jpg" alt="图片二"></p><h3 id="压栈出栈过程"><a href="#压栈出栈过程" class="headerlink" title="压栈出栈过程"></a>压栈出栈过程</h3><p>当方法运行过程中需要创建局部变量时，就将局部变量的值存入栈帧中的局部变量表中。</p><p>Java 虚拟机栈的栈顶的栈帧是当前正在执行的活动栈，也就是当前正在执行的方法，PC 寄存器也会指向这个地址。只有这个活动的栈帧的本地变量可以被操作数栈使用，当在这个栈帧中调用另一个方法，与之对应的栈帧又会被创建，新创建的栈帧压入栈顶，变为当前的活动栈帧。</p><p>方法结束后，当前栈帧被移出，栈帧的返回值变成新的活动栈帧中操作数栈的一个操作数。如果没有返回值，那么新的活动栈帧中操作数栈的操作数没有变化。</p><blockquote><p>由于Java 虚拟机栈是与线程对应的，数据不是线程共享的，因此不用关心数据一致性问题，也不会存在同步锁的问题。</p></blockquote><h3 id="Java-虚拟机栈的特点"><a href="#Java-虚拟机栈的特点" class="headerlink" title="Java 虚拟机栈的特点"></a>Java 虚拟机栈的特点</h3><ul><li>局部变量表随着栈帧的创建而创建，它的大小在编译时确定，创建时只需分配事先规定的大小即可。在方法运行过程中，局部变量表的大小不会发生改变。</li><li>Java 虚拟机栈会出现两种异常：StackOverFlowError 和 OutOfMemoryError。</li><li>StackOverFlowError  若 Java 虚拟机栈的大小不允许动态扩展，那么当线程请求栈的深度超过当前 Java 虚拟机栈的最大深度时，抛出 StackOverFlowError 异常。</li><li>OutOfMemoryError  若允许动态扩展，那么当线程请求栈时内存用完了，无法再动态扩展时，抛出 OutOfMemoryError 异常。</li><li>Java 虚拟机栈也是线程私有，随着线程创建而创建，随着线程的结束而销毁。</li></ul><blockquote><p>出现 StackOverFlowError 时，内存空间可能还有很多。</p></blockquote><h2 id="本地方法栈（C-栈）"><a href="#本地方法栈（C-栈）" class="headerlink" title="本地方法栈（C 栈）"></a>本地方法栈（C 栈）</h2><h3 id="本地方法栈的定义"><a href="#本地方法栈的定义" class="headerlink" title="本地方法栈的定义"></a>本地方法栈的定义</h3><p>本地方法栈是为 JVM 运行 Native 方法准备的空间，由于很多 Native 方法都是用 C 语言实现的，所以它通常又叫 C 栈。它与 Java 虚拟机栈实现的功能类似，只不过本地方法栈是描述本地方法运行过程的内存模型。</p><h3 id="栈帧变化过程"><a href="#栈帧变化过程" class="headerlink" title="栈帧变化过程"></a>栈帧变化过程</h3><p>本地方法被执行时，在本地方法栈也会创建一块栈帧，用于存放该方法的局部变量表、操作数栈、动态链接、方法出口信息等。</p><p>方法执行结束后，相应的栈帧也会出栈，并释放内存空间。也会抛出 StackOverFlowError 和 OutOfMemoryError 异常。</p><blockquote><p>如果 Java 虚拟机本身不支持 Native 方法，或是本身不依赖于传统栈，那么可以不提供本地方法栈。如果支持本地方法栈，那么这个栈一般会在线程创建的时候按线程分配。</p></blockquote><h2 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h2><h3 id="堆的定义"><a href="#堆的定义" class="headerlink" title="堆的定义"></a>堆的定义</h3><p>堆是用来存放对象的内存空间，几乎所有的对象都存储在堆中。</p><h3 id="堆的特点"><a href="#堆的特点" class="headerlink" title="堆的特点"></a>堆的特点</h3><ul><li>线程共享，整个 Java 虚拟机只有一个堆，所有的线程都访问同一个堆。而程序计数器、Java 虚拟机栈、本地方法栈都是一个线程对应一个。</li><li>在虚拟机启动时创建。</li><li>是垃圾回收的主要场所。</li><li>进一步可分为：新生代(Eden区  From Survior  To Survivor)、老年代。</li></ul><p>不同的区域存放不同生命周期的对象，这样可以根据不同的区域使用不同的垃圾回收算法，更具有针对性。</p><p>堆的大小既可以固定也可以扩展，但对于主流的虚拟机，堆的大小是可扩展的，因此当线程请求分配内存，但堆已满，且内存已无法再扩展时，就抛出 OutOfMemoryError 异常。</p><blockquote><p>Java 堆所使用的内存不需要保证是连续的。而由于堆是被所有线程共享的，所以对它的访问需要注意同步问题，方法和对应的属性都需要保证一致性。</p></blockquote><h2 id="方法区"><a href="#方法区" class="headerlink" title="方法区"></a>方法区</h2><h3 id="方法区的定义"><a href="#方法区的定义" class="headerlink" title="方法区的定义"></a>方法区的定义</h3><p>Java 虚拟机规范中定义方法区是堆的一个逻辑部分。方法区存放以下信息：  </p><ul><li>已经被虚拟机加载的类信息</li><li>常量</li><li>静态变量</li><li>即时编译器编译后的代码</li></ul><h3 id="方法区的特点"><a href="#方法区的特点" class="headerlink" title="方法区的特点"></a>方法区的特点</h3><ul><li>线程共享。  方法区是堆的一个逻辑部分，因此和堆一样，都是线程共享的。整个虚拟机中只有一个方法区。</li><li>永久代。  方法区中的信息一般需要长期存在，而且它又是堆的逻辑分区，因此用堆的划分方法，把方法区称为“永久代”。</li><li>内存回收效率低。  方法区中的信息一般需要长期存在，回收一遍之后可能只有少量信息无效。主要回收目标是：对常量池的回收；对类型的卸载。</li><li>Java 虚拟机规范对方法区的要求比较宽松。  和堆一样，允许固定大小，也允许动态扩展，还允许不实现垃圾回收。</li></ul><h3 id="运行时常量池"><a href="#运行时常量池" class="headerlink" title="运行时常量池"></a>运行时常量池</h3><p>方法区中存放：类信息、常量、静态变量、即时编译器编译后的代码。常量就存放在运行时常量池中。</p><p>当类被 Java 虚拟机加载后， .class 文件中的常量就存放在方法区的运行时常量池中。而且在运行期间，可以向常量池中添加新的常量。如 String 类的 intern() 方法就能在运行期间向常量池中添加字符串常量。</p><h2 id="直接内存（堆外内存）"><a href="#直接内存（堆外内存）" class="headerlink" title="直接内存（堆外内存）"></a>直接内存（堆外内存）</h2><p>直接内存是除 Java 虚拟机之外的内存，但也可能被 Java 使用。</p><h3 id="操作直接内存"><a href="#操作直接内存" class="headerlink" title="操作直接内存"></a>操作直接内存</h3><p>在 NIO 中引入了一种基于通道和缓冲的 IO 方式。它可以通过调用本地方法直接分配 Java 虚拟机之外的内存，然后通过一个存储在堆中的<code>DirectByteBuffer</code>对象直接操作该内存，而无须先将外部内存中的数据复制到堆中再进行操作，从而提高了数据操作的效率。</p><p>直接内存的大小不受 Java 虚拟机控制，但既然是内存，当内存不足时就会抛出 OutOfMemoryError 异常。</p><h3 id="直接内存与堆内存比较"><a href="#直接内存与堆内存比较" class="headerlink" title="直接内存与堆内存比较"></a>直接内存与堆内存比较</h3><ul><li>直接内存申请空间耗费更高的性能</li><li>直接内存读取 IO 的性能要优于普通的堆内存。</li><li>直接内存作用链： 本地 IO -&gt; 直接内存 -&gt; 本地 IO</li><li>堆内存作用链：本地 IO -&gt; 直接内存 -&gt; 非直接内存 -&gt; 直接内存 -&gt; 本地 IO</li></ul><blockquote><p>服务器管理员在配置虚拟机参数时，会根据实际内存设置<code>-Xmx</code>等参数信息，但经常忽略直接内存，使得各个内存区域总和大于物理内存限制，从而导致动态扩展时出现<code>OutOfMemoryError</code>异常。</p></blockquote><hr><p><strong>持续更新中…，如果遇到问题欢迎联系我，在文章最后评论区【留言和讨论】，当然，欢迎点击文章最后的打赏按键，请博主一杯冰阔乐，笑～</strong></p><table>  <tbody><tr>    <td><img width="100" src="https://hualei.online/medias/reward/alipay.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/wechat.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/zan.png"></td>     </tr></tbody></table><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 技术学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 互联网 </tag>
            
            <tag> Java </tag>
            
            <tag> JVM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop极简入门</title>
      <link href="/posts/0.html"/>
      <url>/posts/0.html</url>
      
        <content type="html"><![CDATA[<p>其实Hadoop诞生至今已经十多年了，网络上也充斥着关于Hadoop相关知识的的海量资源。但是，有时还是会使刚刚接触大数据领域的童鞋分不清hadoop、hdfs、Yarn和MapReduce等等技术词汇。</p><p>Hadoop是ASF(Apache软件基金会)开源的，根据Google开源的三篇大数据论文设计的，一个能够允许大量数据在计算机集群中，通过使用简单的编程模型进行分布式处理的框架。其设计的规模可从单一的服务器到数千台服务器，每一个均可提供局部运算和存储功能。Hadoop并不依赖昂贵的硬件以支持高可用性。Hadoop可以检测并处理应用层上的错误，并可以把错误转移到其他服务器上(让它错误，我在用别的服务器顶上就可以了)，所以Hadoop提供一个基于计算机集群的、高效性的服务。</p><p>经过十年的发展，Hadoop这个名词的本身也在不断进化者，目前我们提到Hadoop大多是指大数据的生态圈，这个生态圈包括众多的软件技术(e.g.  HBase、Hive和Spark等等)。</p><p>有如Spring框架有着最基础的几个模块Context、Bean和Core，其他的模块和项目都是基于这些基础模块构建。Hadoop与之一样，也有最基础的几个模块。</p><p><strong>Common</strong>: 支持其他模块的公用工具包。</p><p><strong>HDFS</strong>: 一个可高吞吐访问应用数据的分布式文件系统。</p><p><strong>YARN</strong>: 一个管理集群服务器资源和任务调度的框架。</p><p><strong>MapReduce</strong>: 基于Yarn对大数据集进行并行计算的系统。</p><p>其它的，像HBase、Hive等等不过在这几个基础模块上的高级抽象。另外Hadoop也不是目前大数据的唯一解决方案，像Amazon的大数据技术方案等等。</p><p>Common<br>Common模块是Hadoop最为基础的模块，他为其他模块提供了像操作文件系统、I/O、序列化和远程方法调用等最为基础的实现。如果想深入的了解Hadoop的具体实现，可以阅读一下Common的源码。</p><h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><p>HDFS是“Hadoop Distributed File System”的首字母缩写，是一种设计运行在一般硬件条件（不需要一定是服务器级别的设备，但更好的设备能发挥更大的作用）下的分布式文件系统. 他和现有的其他分布式文件系统(e.g. RAID)有很多相似的地方。和其他分布式文件系统的不同之处是HDFS设计为运行在低成本的硬件上(e.g. 普通的PC机)，且提供高可靠性的服务器. HDFS设计满足大数据量，大吞吐量的应用情况。</p><p>为了更好的理解分布式文件系统，我们先从文件讲起。</p><h3 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h3><p>文件这个词，恐怕只要是现代人都不会陌生。但是在不同行业中，文件有着不同的意义。在计算机科学领域，文件是什么呢？文件是可以在目录中看的见的图标么？当然不是。文件在存储设备时，是个N长的字节序列。而在一个计算机使用者的角度而言，文件是对所有I/O设备的抽象。每个I/O设备都可以视为文件，包括磁盘、键盘和网络等。文件这个简单而精致的概念其内涵是十分丰富的，它向应用程序提供了一个统一的视角，来看待系统中可能含有的各式各样的I/O设备。</p><h3 id="文件系统"><a href="#文件系统" class="headerlink" title="文件系统"></a>文件系统</h3><p>那么一台计算机上肯定不止一个文件，成千上万的文件怎么管理呢？因此需要我们需要一种对文件进行管理的东西，即文件系统。文件系统是一种在计算机上存储和组织数据的方法，它使得对其访问和查找变得容易，文件系统使用文件和树形目录的抽象逻辑概念代替了硬盘和光盘等物理设备使用数据块的概念，用户使用文件系统来保存数据而不必关心数据实际保存在硬盘的地址为多少的数据块上，只需要记住这个文件的所属目录和文件名。在写入新数据之前，用户不必关心硬盘上的那个块地址没有被使用，硬盘上的存储空间管理(分配和释放)功能由文件系统自动完成，用户只需要记住数据被写入到了哪个文件中即可。</p><h3 id="分布式文件系统"><a href="#分布式文件系统" class="headerlink" title="分布式文件系统"></a>分布式文件系统</h3><p>相对于单机的文件系统而言，分布式文件系统(Distributed file system)。是一种允许文件通过网络在多台主机上分享的文件系统，可让多计算机上的多用户分享文件和存储空间。</p><p>在这样的文件系统中，客户端并非直接访问底层的数据存储区块和磁盘。而是通过网络，基于单机文件系统并借由特定的通信协议的帮助，来实现对于文件系统的读写。</p><p>分布式文件系统需要拥有的最基本的能力是通过畅通网络I/O来实现数据的复制与容错。也就是说，一方面一个文件是分为多个数据块分布在多个设备中。另一方面，数据块有多个副本分布在不同的设备上。即使有一小部分的设备出现离线和宕机等情况，整体来说文件系统仍然可以持续运作而不会有数据损失。</p><p>注意:分布式文件系统和分布式数据存储的界线是模糊的，但一般来说，分布式文件系统是被设计用在局域网，比较强调的是传统文件系统概念的延伸，并通过软件方法来达成容错的目的。而分布式数据存储，则是泛指应用分布式运算技术的文件和数据库等提供数据存储服务的系统。</p><h3 id="HDFS-1"><a href="#HDFS-1" class="headerlink" title="HDFS"></a>HDFS</h3><p>HDFS正是Hadoop中负责分布式文件系统的。HDFS采用master/slave架构。一个HDFS集群是由一个Namenode和一定数目的Datanodes组成。Namenode是一个中心服务器，负责管理文件系统的命名空间以及文件的访问控制。集群中的Datanode一般是一个设备上部署一个，负责管理它所在节点上的存储。HDFS暴露了文件系统的命名空间，用户能够以文件的形式在上面存储数据。实际上，一个文件会被分成一个或多个数据块，这些块存储在一组Datanode上。Namenode执行文件系统的命名空间操作，比如打开、关闭、重命名文件或目录。它也负责确定数据块到具体Datanode设备的映射。Datanode负责处理文件系统客户端的读写请求。在Namenode的统一调度下进行数据块的创建、删除和复制。为了保证文件系统的高可靠，往往需要另一个Standby的Namenode在Actived Namenode出现问题后，立刻接管文件系统。</p><p>网络上有很多关于hdfs的安装配置手册，本文就不再复述。只提供一个以前项目中应用过的部署架构仅供大家参考。</p><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/Hadoop%E6%9E%81%E7%AE%80%E5%85%A5%E9%97%A8/%E5%9B%BE%E7%89%87%E4%B8%80.jpg" alt="图片一"></p><p>这个高可用的HDFS架构是由3台zookeeper设备、2台域名服务(DNS)和时间服务(NTP)设备、2台Namenode设备(如果必要Standby可以更多)、一个共享存储设备(NFS)和N个DataNode组成。</p><p>Zookeeper负责接受NameNode的心跳，当Actived namenode不向zookeeper报告心跳时，Standby Namenode的监控进程会收到这个消息，从而激活Standby NameNode并接管Active NameNode的工作。</p><p>NFS负责为2个NameNode存储EditLog文件，(NameNode 在执行 HDFS 客户端提交的创建文件或者移动文件这样的写操作时，会首先把这些操作记录在 EditLog 文件之中，然后再更新内存中的文件系统镜像，最终再刷新到磁盘。 EditLog 只是在数据恢复的时候起作用。记录在 EditLog 之中的每一个操作又称为一个事务，每个事务有一个整数形式的事务 id 作为编号。EditLog 会被切割为很多段，每一段称为一个 Segment)当发生NameNode切换的情况时，Standby NameNode接管后，会根据EditLog中把未完成的写操作继续下去并开使向EditLog写入新的写操作记录。(此外，hadoop还提供了另一种QJM的EditLog方案)</p><p>DNS&amp;NTP分布负责整个系统的(包括客户端)域名服务和时间服务。这个在集群部署中是非常有必要的两个存在。首先说一下DNS的必要性，一、Hadoop是极力提倡用机器名作为在HDFS环境中的标识。二、当然可以在/etc/hosts文件中去标明机器名和IP的映射关系，可是请想想如果在一个数千台设备的集群中添加一个设备时，负责系统维护的伙伴会不会骂集群的设计者呢？其次是NTP的必要性，在刚刚开始接触Hadoop集群时我遇到的大概90%的问题是由于各个设备时间不一致导致的。各个设备的时间同步是数据一致性和管理一致性的一个基本保障。</p><h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><p>MapReduce是一个使用简单的软件框架，基于它写出来的应用程序能够运行在由上千个商用机器组成的大型集群上，并以一种可靠容错的方式并行处理上T级别的数据集。</p><p>一个MapReduce 作业(job)通常会把输入的数据集切分为若干独立的数据块，由 map任务(task)以完全并行的方式处理它们。框架会对map的输出先进行排序， 然后把结果输入给reduce任务。通常作业的输入和输出都会被存储在文件系统中。 整个框架负责任务的调度和监控，以及重新执行已经失败的任务。</p><p>通常，MapReduce框架和HDFS是运行在一相同的设备集群上的，也就是说，计算设备和存储设备通常在一起。这种配置允许框架在那些已经存好数据的设备上高效地调度任务，这可以使整个集群的网络带宽被非常高效地利用。</p><p>MapReduce框架由一个单独的master JobTracker 和每个集群设备一个slave TaskTracker共同组成。master负责调度构成一个作业的所有任务，这些任务分布在不同的slave上，master监控它们的执行，重新执行已经失败的任务。而slave仅负责执行由master指派的任务。</p><p>用户编写的MapReduce应用程序应该指明输入/输出的文件位置(路径)，并通过实现合适的接口或抽象类提供map和reduce函数。再加上其他作业的参数，就构成了作业配置(job configuration)。然后，job client提交作业(jar包/可执行程序等)和配置信息给JobTracker，后者负责分发这些软件和配置信息给slave、调度任务并监控它们的执行，同时提供状态和诊断信息给job-client。</p><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/Hadoop%E6%9E%81%E7%AE%80%E5%85%A5%E9%97%A8/%E5%9B%BE%E7%89%87%E4%BA%8C.jpg" alt="图片二"></p><p>在抽象的层面上MapReduce是由两个函数Map和Reduce组成的。简单来说，一个Map函数就是对一些独立元素组成的概念上的列表的每一个元素进行指定的操作。事实上，每个元素都是被独立操作的，而原始列表没有被更改，因为这里创建了一个新的列表来保存操作结果。这就是说，Map操作是可以高度并行的。而Reduce函数指的是对Map函数的结果（中间经过洗牌的过程，会把map的结果进行分组）分组后多个列表的元素进行适当的归并。</p><p>注意:虽然Hadoop框架是用JavaTM实现的，但MapReduce应用程序则不一定要用 Java来写 。至少Scala是可以写的哟。</p><p>附上Scala实现的计算词频的Scala源码</p><pre><code>import java.io.IOExceptionimport java.util.StringTokenizerimport org.apache.hadoop.conf.Configurationimport org.apache.hadoop.fs.Pathimport org.apache.hadoop.io.{IntWritable, Text}import org.apache.hadoop.mapreduce.lib.input.FileInputFormatimport org.apache.hadoop.mapreduce.lib.output.FileOutputFormatimport org.apache.hadoop.mapreduce.{Job, Mapper, Reducer}import scala.collection.JavaConversionsobject WordCount {def main(args: Array[String]): Unit = {val job = new Job(new Configuration(), "WordCount")job.setJarByClass(classOf[WordMapper]);job.setMapperClass(classOf[WordMapper]);job.setCombinerClass(classOf[WordReducer]);job.setReducerClass(classOf[WordReducer]);job.setOutputKeyClass(classOf[Text]);job.setOutputValueClass(classOf[IntWritable]);job.setNumReduceTasks(1)FileInputFormat.addInputPath(job, new Path(args(0)));FileOutputFormat.setOutputPath(job, new Path(args(1)));System.exit(job.waitForCompletion(true) match { case true =&gt; 0case false =&gt; 1});}}class WordMapper extends Mapper[Object, Text, Text, IntWritable] {val one = new IntWritable(1)@throws[IOException]@throws[InterruptedException]override def map(key: Object, value: Text, context: Mapper[Object, Text, Text, IntWritable]#Context) = {val stringTokenizer = new StringTokenizer(value.toString());while (stringTokenizer.hasMoreTokens()) {context.write(new Text(stringTokenizer.nextToken()), one);}}}class WordReducer extends Reducer[Text, IntWritable, Text, IntWritable] {@throws[IOException]@throws[InterruptedException]override def reduce(key: Text, values: java.lang.Iterable[IntWritable], context: Reducer[Text, IntWritable, Text, IntWritable]#Context) = {import JavaConversions.iterableAsScalaIterablecontext.write(key, new IntWritable(values.map(x=&gt;x.get()).reduce(_+_)));}}</code></pre><h3 id="Yarn"><a href="#Yarn" class="headerlink" title="Yarn"></a>Yarn</h3><p>YARN(Yet Another Resource Negotiator)是Hadoop的设备资源管理器，它是一个通用资源管理系统，MapReduce和其他上层应用提供统一的资源管理和调度，它为集群在利用率、资源统一管理和数据共享等方面提供了巨大的帮助。</p><p>Yarn由ResourceManager、NodeManager、ApplicationMaster和Containe四个概念构成。</p><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/Hadoop%E6%9E%81%E7%AE%80%E5%85%A5%E9%97%A8/%E5%9B%BE%E7%89%87%E4%B8%89.jpg" alt="图片三"></p><p>ResourceManager是一个全局的资源管理器，负责整个系统的资源管理和分配。它主要由两个组件构成:调度器(Scheduler)和应用程序管理器(Applications Manager)。调度器根据容量、队列等限制条件，将系统中的资源分配给各个正在运行的MapReduce程序。应用程序管理器负责管理整个系统中所有MapReduce程序，包括提交、与调度器协商资源以启动ApplicationMaster、监控ApplicationMaster运行状态并在失败时重新启动它等。</p><p>用户提交的每个MapReduce程序均包含一个ApplicationMaster，主要功能包括：与ResourceManager调度器协商以获取资源(用Container表示)；将得到的任务进一步分配给内部的任务(资源的二次分配)；与NodeManager通信以启动/停止任务；监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务。</p><p>NodeManager是每个设备上的资源和任务管理器，一方面，它会定时地向ResourceManager汇报本设备上的资源使用情况和各个Container的运行状态；另一方面，它接收并处理来自ApplicationMaster的Container启动/停止等各种请求。</p><p>Container是YARN中的资源抽象，它封装了某个设备上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM申请资源时，RM为AM返回的资源便是用Container表示。</p><h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>本文走马观花的介绍了Hadoop相关内容。文章的主要目的是给大家一个对大数据的分布式解决方案的感官印象，为后面的大数据相关文章提供一个基础的理解。最后要强调的是，思考大数据方向的问题是一定要记住分布式的概念，因为你的数据并不在一个设备中甚至不再一个集群中，而且计算也是分布的。所以在设计大数据应用程序时，要花时间思考程序和算法在单机应用和分布式应用所产生的不同(e.g. 加权平均值)。</p><p><strong>持续更新中…，如果遇到问题欢迎联系我，在文章最后评论区【留言和讨论】，当然，欢迎点击文章最后的打赏按键，请博主一杯冰阔乐，笑～</strong></p><table>  <tbody><tr>    <td><img width="100" src="https://hualei.online/medias/reward/alipay.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/wechat.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/zan.png"></td>     </tr></tbody></table><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 技术学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> 互联网 </tag>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux基础和命令</title>
      <link href="/posts/1792.html"/>
      <url>/posts/1792.html</url>
      
        <content type="html"><![CDATA[<h2 id="Linux命令"><a href="#Linux命令" class="headerlink" title="Linux命令"></a>Linux命令</h2><p>我是小白，我从来没玩过Linux,请点这里：</p><pre><code>http://www.runoob.com/linux/Linux-intro.html</code></pre><h2 id="必须学会的命令"><a href="#必须学会的命令" class="headerlink" title="必须学会的命令"></a>必须学会的命令</h2><h3 id="1-0-man和page"><a href="#1-0-man和page" class="headerlink" title="1.0 man和page"></a>1.0 man和page</h3><pre><code>1.内部命令：echo查看内部命令帮助：help echo 或者 man echo2.外部命令：ls查看外部命令帮助：ls --help 或者 man ls 或者 info ls3.man文档的类型(1~9)man 7 manman 5 passwd4.快捷键：ctrl + c：停止进程ctrl + l：清屏ctrl + r：搜索历史命令ctrl + q：退出5.善于用tab键</code></pre><h3 id="2-0-常用"><a href="#2-0-常用" class="headerlink" title="2.0. 常用"></a>2.0. 常用</h3><pre><code>说明：安装linux时，创建一个hadoop用户，然后使用root用户登陆系统1.进入到用户根目录cd ~ 或 cd2.查看当前所在目录pwd3.进入到hadoop用户根目录cd ~hadoop4.返回到原来目录cd -5.返回到上一级目录cd ..6.查看hadoop用户根目录下的所有文件ls -la7.在根目录下创建一个hadoop的文件夹mkdir /hadoop8.在/hadoop目录下创建src和WebRoot两个文件夹分别创建：mkdir /hadoop/src          mkdir /hadoop/WebRoot同时创建：mkdir /hadoop/{src,WebRoot}进入到/hadoop目录，在该目录下创建.classpath和README文件分别创建：touch .classpath          touch README同时创建：touch {.classpath,README}查看/hadoop目录下面的所有文件ls -la在/hadoop目录下面创建一个test.txt文件,同时写入内容"this is test"echo "this is test" &gt; test.txt查看一下test.txt的内容cat test.txtmore test.txtless test.txt向README文件追加写入"please read me first"echo "please read me first" &gt;&gt; README将test.txt的内容追加到README文件中cat test.txt &gt;&gt; README拷贝/hadoop目录下的所有文件到/hadoop-bakcp -r /hadoop /hadoop-bak进入到/hadoop-bak目录，将test.txt移动到src目录下，并修改文件名为Student.javamv test.txt src/Student.java在src目录下创建一个struts.xml&gt; struts.xml删除所有的xml类型的文件rm -rf *.xml删除/hadoop-bak目录和下面的所有文件rm -rf /hadoop-bak返回到/hadoop目录，查看一下README文件有多单词，多少个少行wc -w READMEwc -l README返回到根目录，将/hadoop目录先打包，再用gzip压缩分步完成：tar -cvf hadoop.tar hadoop          gzip hadoop.tar一步完成：tar -zcvf hadoop.tar.gz hadoop将其解压缩，再取消打包分步完成：gzip -d hadoop.tar.gz 或 gunzip hadoop.tar.gz一步完成：tar -zxvf hadoop.tar.gz将/hadoop目录先打包，同时用bzip2压缩，并保存到/tmp目录下tar -jcvf /tmp/hadoop.tar.bz2 hadoop将/tmp/hadoop.tar.bz2解压到/usr目录下面tar -jxvf hadoop.tar.bz2 -C /usr/</code></pre><h4 id="2-1-文件命令"><a href="#2-1-文件命令" class="headerlink" title="2.1 文件命令"></a>2.1 文件命令</h4><pre><code>1.进入到用户根目录cd ~ 或者 cdcd ~hadoop回到原来路径cd -2.查看文件详情stat a.txt3.移动mv a.txt /ect/改名mv b.txt a.txt移动并改名mv a.txt ../b.txt4拷贝并改名cp a.txt /etc/b.txt5.vi撤销修改ctrl + u (undo)恢复ctrl + r (redo)6.名令设置别名(重启后无效)alias ll="ls -l"取消unalias ll7.如果想让别名重启后仍然有效需要修改vi ~/.bashrc8.添加用户useradd hadooppasswd hadoop9创建多个文件touch a.txt b.txttouch /home/{a.txt,b.txt}10.将一个文件的内容复制到里另一个文件中cat a.txt &gt; b.txt追加内容cat a.txt &gt;&gt; b.txt 11.将a.txt 与b.txt设为其拥有者和其所属同一个组者可写入，但其他以外的人则不可写入:chmod ug+w,o-w a.txt b.txtchmod a=wx c.txt12.将当前目录下的所有文件与子目录皆设为任何人可读取:chmod -R a+r *13.将a.txt的用户拥有者设为users,组的拥有者设为jessie:chown users:jessie a.txt14.将当前目录下的所有文件与子目录的用户的使用者为lamport,组拥有者皆设为users，chown -R lamport:users *15.将所有的java语言程式拷贝至finished子目录中:cp *.java finished16.将目前目录及其子目录下所有扩展名是java的文件列出来。find -name "*.java"查找当前目录下扩展名是java 的文件find -name *.java17.删除当前目录下扩展名是java的文件rm -f *.java</code></pre><h4 id="2-2-系统命令"><a href="#2-2-系统命令" class="headerlink" title="2.2 系统命令"></a>2.2 系统命令</h4><pre><code>1.查看主机名hostname2.修改主机名(重启后无效)hostname hadoop3.修改主机名(重启后永久生效)vi /ect/sysconfig/network4.修改IP(重启后无效)ifconfig eth0 192.168.12.225.修改IP(重启后永久生效)vi /etc/sysconfig/network-scripts/ifcfg-eth06.查看系统信息uname -auname -r7.查看ID命令id -uid -g8.日期datedate +%Y-%m-%ddate +%Tdate +%Y-%m-%d" "%T9.日历cal 201210.查看文件信息file filename11.挂载硬盘mountumount加载windows共享mount -t cifs //192.168.1.100/tools /mnt12.查看文件大小du -hdu -ah13.查看分区df -h14.sshssh hadoop@192.168.1.115.关机shutdown -h now /init 0shutdown -r now /reboot</code></pre><h4 id="2-3-用户和组"><a href="#2-3-用户和组" class="headerlink" title="2.3 用户和组"></a>2.3 用户和组</h4><pre><code>添加一个tom用户，设置它属于users组，并添加注释信息分步完成：useradd tom          usermod -g users tom          usermod -c "hr tom" tom一步完成：useradd -g users -c "hr tom" tom设置tom用户的密码passwd tom修改tom用户的登陆名为tomcatusermod -l tomcat tom将tomcat添加到sys和root组中usermod -G sys,root tomcat查看tomcat的组信息groups tomcat添加一个jerry用户并设置密码useradd jerrypasswd jerry添加一个交america的组groupadd america将jerry添加到america组中usermod -g america jerry将tomcat用户从root组和sys组删除gpasswd -d tomcat rootgpasswd -d tomcat sys将america组名修改为amgroupmod -n am america</code></pre><h4 id="2-4-权限"><a href="#2-4-权限" class="headerlink" title="2.4 权限"></a>2.4 权限</h4><pre><code>创建a.txt和b.txt文件，将他们设为其拥有者和所在组可写入，但其他以外的人则不可写入:chmod ug+w,o-w a.txt b.txt创建c.txt文件所有人都可以写和执行chmod a=wx c.txt 或chmod 666 c.txt将/hadoop目录下的所有文件与子目录皆设为任何人可读取chmod -R a+r /hadoop将/hadoop目录下的所有文件与子目录的拥有者设为root，用户拥有组为userschown -R root:users /hadoop将当前目录下的所有文件与子目录的用户皆设为hadoop，组设为userschown -R hadoop:users *</code></pre><h4 id="2-5-目录属性"><a href="#2-5-目录属性" class="headerlink" title="2.5 目录属性"></a>2.5 目录属性</h4><pre><code>1.查看文件夹属性ls -ld test2.文件夹的rwx--x:可以cd进去r-x:可以cd进去并ls-wx:可以cd进去并touch，rm自己的文件，并且可以vi其他用户的文件-wt:可以cd进去并touch，rm自己的文件ls -ld /tmpdrwxrwxrwt的权限值是1777(sticky)</code></pre><h4 id="2-6-软件安装"><a href="#2-6-软件安装" class="headerlink" title="2.6 软件安装"></a>2.6 软件安装</h4><pre><code>1.安装JDK    *添加执行权限         chmod u+x jdk-6u45-linux-i586.bin    *解压        ./jdk-6u45-linux-i586.bin    *在/usr目录下创建java目录        mkdir /usr/java    *将/soft目录下的解压的jdk1.6.0_45剪切到/usr/java目录下        mv jdk1.6.0_45/ /usr/java/    *添加环境变量        vim /etc/profile        *在/etc/profile文件最后添加            export JAVA_HOME=/usr/java/jdk1.6.0_45            export CLASSPATH=$JAVA_HOME/lib            export PATH=$PATH:$JAVA_HOME/bin    *更新配置        source /etc/profile2.安装tomcat    tar -zxvf /soft/apache-tomcat-7.0.47.tar.gz -C /programs/    cd /programs/apache-tomcat-7.0.47/bin/    ./startup.sh3.安装eclipse</code></pre><h4 id="2-7-vim"><a href="#2-7-vim" class="headerlink" title="2.7 vim"></a>2.7 vim</h4><pre><code>ia/Ao/Or + ?替换0:文件当前行的开头$:文件当前行的末尾G:文件的最后一行开头1 + G到第一行 9 + G到第九行 = :9dd:删除一行3dd：删除3行yy:复制一行3yy:复制3行p:粘贴u:undoctrl + r:redo"a剪切板a"b剪切板b"ap粘贴剪切板a的内容每次进入vi就有行号vi ~/.vimrcset nu:w a.txt另存为:w &gt;&gt; a.txt内容追加到a.txt:e!恢复到最初状态:1,$s/hadoop/root/g 将第一行到追后一行的hadoop替换为root:1,$s/hadoop/root/c 将第一行到追后一行的hadoop替换为root(有提示)</code></pre><h4 id="2-8-查找"><a href="#2-8-查找" class="headerlink" title="2.8 查找"></a>2.8 查找</h4><pre><code>1.查找可执行的命令：which ls2.查找可执行的命令和帮助的位置：whereis ls3.查找文件(需要更新库:updatedb)locate hadoop.txt4.从某个文件夹开始查找find / -name "hadooop*"find / -name "hadooop*" -ls5.查找并删除find / -name "hadooop*" -ok rm {} \;find / -name "hadooop*" -exec rm {} \;6.查找用户为hadoop的文件find /usr -user hadoop -ls7.查找用户为hadoop并且(-a)拥有组为root的文件find /usr -user hadoop -a -group root -ls8.查找用户为hadoop或者(-o)拥有组为root并且是文件夹类型的文件find /usr -user hadoop -o -group root -a -type d9.查找权限为777的文件find / -perm -777 -type d -ls10.显示命令历史history11.grepgrep hadoop /etc/password</code></pre><h4 id="2-9-打包与压缩"><a href="#2-9-打包与压缩" class="headerlink" title="2.9 打包与压缩"></a>2.9 打包与压缩</h4><pre><code>1.gzip压缩gzip a.txt2.解压gunzip a.txt.gzgzip -d a.txt.gz3.bzip2压缩bzip2 a4.解压bunzip2 a.bz2bzip2 -d a.bz25.将当前目录的文件打包tar -cvf bak.tar .将/etc/password追加文件到bak.tar中(r)tar -rvf bak.tar /etc/password6.解压tar -xvf bak.tar7.打包并压缩gziptar -zcvf a.tar.gz8.解压缩tar -zxvf a.tar.gz解压到/usr/下tar -zxvf a.tar.gz -C /usr9.查看压缩包内容tar -ztvf a.tar.gzzip/unzip10.打包并压缩成bz2tar -jcvf a.tar.bz211.解压bz2tar -jxvf a.tar.bz2</code></pre><h4 id="2-10-正则"><a href="#2-10-正则" class="headerlink" title="2.10 正则"></a>2.10 正则</h4><pre><code>1.cut截取以:分割保留第七段grep hadoop /etc/passwd | cut -d: -f72.排序du | sort -n 3.查询不包含hadoop的grep -v hadoop /etc/passwd4.正则表达包含hadoopgrep 'hadoop' /etc/passwd5.正则表达(点代表任意一个字符)grep 'h.*p' /etc/passwd6.正则表达以hadoop开头grep '^hadoop' /etc/passwd7.正则表达以hadoop结尾grep 'hadoop$' /etc/passwd规则：.  : 任意一个字符a* : 任意多个a(零个或多个a)a? : 零个或一个aa+ : 一个或多个a.* : 任意多个任意字符\. : 转义.\&lt;h.*p\&gt; ：以h开头，p结尾的一个单词o\{2\} : o重复两次grep '^i.\{18\}n$' /usr/share/dict/words查找不是以#开头的行grep -v '^#' a.txt | grep -v '^$' 以h或r开头的grep '^[hr]' /etc/passwd不是以h和r开头的grep '^[^hr]' /etc/passwd不是以h到r开头的grep '^[^h-r]' /etc/passwd</code></pre><h4 id="2-11-输入输出重定向及管道"><a href="#2-11-输入输出重定向及管道" class="headerlink" title="2.11 输入输出重定向及管道"></a>2.11 输入输出重定向及管道</h4><pre><code>1.新建一个文件touch a.txt&gt; b.txt2.错误重定向:2&gt;find /etc -name zhaoxing.txt 2&gt; error.txt3.将正确或错误的信息都输入到log.txt中find /etc -name passwd &gt; /tmp/log.txt 2&gt;&amp;1 find /etc -name passwd &amp;&gt; /tmp/log.txt4.追加&gt;&gt;5.将小写转为大写（输入重定向）tr "a-z" "A-Z" &lt; /etc/passwd6.自动创建文件cat &gt; log.txt &lt;&lt; EXIT&gt; ccc&gt; ddd&gt; EXI7.查看/etc下的文件有多少个？ls -l /etc/ | grep '^d' | wc -l8.查看/etc下的文件有多少个，并将文件详情输入到result.txt中ls -l /etc/ | grep '^d' | tee result.txt | wc -l</code></pre><h4 id="2-12-进程控制"><a href="#2-12-进程控制" class="headerlink" title="2.12 进程控制"></a>2.12 进程控制</h4><pre><code>1.查看用户最近登录情况lastlastlog2.查看硬盘使用情况df3.查看文件大小du4.查看内存使用情况free5.查看文件系统/proc6.查看日志ls /var/log/7.查看系统报错日志tail /var/log/messages8.查看进程top9.结束进程kill 1234kill -9 4333</code></pre><hr><p><strong>持续更新中…，如果遇到问题欢迎联系我，在文章最后评论区【留言和讨论】，当然，欢迎点击文章最后的打赏按键，请博主一杯冰阔乐，笑～</strong></p><table>  <tbody><tr>    <td><img width="100" src="https://hualei.online/medias/reward/alipay.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/wechat.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/zan.png"></td>     </tr></tbody></table><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 技术学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 互联网 </tag>
            
            <tag> 技术 </tag>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop2.0完全分布式HA搭建</title>
      <link href="/posts/4175.html"/>
      <url>/posts/4175.html</url>
      
        <content type="html"><![CDATA[<h1 id="1-Hadoop2-0-HA搭建步骤"><a href="#1-Hadoop2-0-HA搭建步骤" class="headerlink" title="1    Hadoop2.0 HA搭建步骤"></a>1    Hadoop2.0 HA搭建步骤</h1><h2 id="1-1-准备工作"><a href="#1-1-准备工作" class="headerlink" title="1.1    准备工作"></a>1.1    准备工作</h2><p>6台虚拟机，内存512M，hadoop1~6<br>修改静态IP：192.168.65.121 ~ 126</p><h2 id="1-2-架构图"><a href="#1-2-架构图" class="headerlink" title="1.2    架构图"></a>1.2    架构图</h2><p> <img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/pictures/1.png" alt="1"></p><h2 id="1-3-集群节点分配"><a href="#1-3-集群节点分配" class="headerlink" title="1.3    集群节点分配"></a>1.3    集群节点分配</h2><p>    hadoop1<br>Zookeeper<br>NameNode(active)<br>Resourcemanager (active)<br>    hadoop2<br>Zookeeper<br>NameNode (standby)<br>    hadoop3<br>Zookeeper<br>ResourceManager(standby)<br>    hadoop4<br>DataNode<br>NodeManager<br>JournalNode<br>    hadoop5<br>DataNode<br>NodeManager<br>JournalNode<br>    hadoop6<br>DataNode<br>NodeManager<br>JournalNode</p><h2 id="1-4-安装步骤"><a href="#1-4-安装步骤" class="headerlink" title="1.4    安装步骤"></a>1.4    安装步骤</h2><h3 id="1-4-1-固化IP"><a href="#1-4-1-固化IP" class="headerlink" title="1.4.1    固化IP"></a>1.4.1    固化IP</h3><p>修改配置文件<br>cd /etc/sysconfig/network-scripts    #进入网络配置目录<br>dir ifcfg*                                #找到网卡配置文件<br>ifcfg-ens16777736  ifcfg-lo<br>vi ifcfg-ens16777736<br>配置文件内容<br>TYPE=Ethernet<br>BOOTPROTO=static                                    #改成static，针对NAT<br>NAME=eno16777736<br>UUID=4cc9c89b-cf9e-4847-b9ea-ac713baf4cc8<br>DEVICE=eno16777736<br>ONBOOT=yes                    #开机启动此网卡<br>IPADDR=192.168.163.129    #固定IP地址<br>NETMASK=255.255.255.0        #子网掩码<br>GATEWAY=192.168.163.2        #网关和NAT自动配置的相同，不同则无法登录<br>DNS1=192.168.163.2            #和网关相同<br>重启网络<br>service network restart<br>ping <a href="http://www.baidu.com" target="_blank" rel="noopener">www.baidu.com</a></p><h3 id="1-4-2-永久关闭每台机器的防火墙"><a href="#1-4-2-永久关闭每台机器的防火墙" class="headerlink" title="1.4.2    永久关闭每台机器的防火墙"></a>1.4.2    永久关闭每台机器的防火墙</h3><p>systemctl stop firewalld.service            #关闭防火墙服务<br>systemctl disable firewalld.service        #禁止防火墙开启启动<br>firewall-cmd –state                        #检查防火墙状态</p><h3 id="1-4-3-为每台机器配置主机名"><a href="#1-4-3-为每台机器配置主机名" class="headerlink" title="1.4.3    为每台机器配置主机名*"></a>1.4.3    为每台机器配置主机名*</h3><p>hadoop1,hadoop2 ……<br>以及hosts文件<br>配置主机名<br>执行：vim /etc/hostname 修改为hadoop1~6<br>然后执行 hostname 主机名<br>达到不重启生效目的<br>配置hosts文件<br>执行：vim /etc/hosts<br>示例：<br>127.0.0.1 localhost<br>::1 localhost<br>192.168.65.121 hadoop1<br>192.168.65.122 hadoop2<br>192.168.65.123 hadoop3<br>192.168.65.124 hadoop4<br>192.168.65.125 hadoop5<br>192.168.65.126 hadoop6</p><h3 id="1-4-4-为每台机器配置ssh免秘钥登录"><a href="#1-4-4-为每台机器配置ssh免秘钥登录" class="headerlink" title="1.4.4    为每台机器配置ssh免秘钥登录"></a>1.4.4    为每台机器配置ssh免秘钥登录</h3><p>执行：ssh-keygen<br>ssh-copy-id root@hadoop1 （分别发送到6台节点上）<br>vim /root/.ssh/known_hosts 检查是否配置成功</p><h3 id="1-4-5-为每台机器安装jdk和配置JAVA-HOME"><a href="#1-4-5-为每台机器安装jdk和配置JAVA-HOME" class="headerlink" title="1.4.5    为每台机器安装jdk和配置JAVA_HOME"></a>1.4.5    为每台机器安装jdk和配置JAVA_HOME</h3><p>vim /etc/profile<br>在尾行添加<br>JAVA_HOME=/home/app/jdk1.8.0_65<br>JAVA_BIN=/home/app/jdk1.8.0_65/bin<br>HADOOP_HOME=/home/app/hadoop-2.7.1<br>PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH<br>CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar<br>export JAVA_HOME JAVA_BIN HADOOP_HOME PATH CLASSPATH</p><h3 id="1-4-6-前三台机器安装和配置zookeeper"><a href="#1-4-6-前三台机器安装和配置zookeeper" class="headerlink" title="1.4.6    前三台机器安装和配置zookeeper"></a>1.4.6    前三台机器安装和配置zookeeper</h3><p>解压安装包 tar -xvf zookeeper-3.4.8.tar.gz<br>进入conf目录 cd zookeeper-3.4.8/conf/<br>复制zoo_sample.cfg 为zoo.cfg:  cp zoo_sample.cfg zoo.cfg<br>编辑zoo.cfg：vim zoo.cfg<br> <img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/pictures/2.png" alt="2"><br> <img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/pictures/3.png" alt="3"><br> <img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/pictures/4.png" alt="4">    </p><p>Zookeeper根目录中创建tmp文件夹，tmp文件夹中创建myid文件，编辑文本：1（2,3）</p><p>拷贝整个zookeeper目录到hadoop2，hadoop3并修改myid文件分别为2,3<br>scp -r zookeeper-3.4.8 hadoop2:/home/app</p><h3 id="1-4-7-安装和配置01节点的hadoop"><a href="#1-4-7-安装和配置01节点的hadoop" class="headerlink" title="1.4.7    安装和配置01节点的hadoop"></a>1.4.7    安装和配置01节点的hadoop</h3><p>创建/home/app目录<br>解压安装包</p><h3 id="1-4-8-配置hadoop-env-sh"><a href="#1-4-8-配置hadoop-env-sh" class="headerlink" title="1.4.8    配置hadoop-env.sh"></a>1.4.8    配置hadoop-env.sh</h3><p>cd etc/hadoop<br>配置 hadoop-env.sh<br>vim hadoop-env.sh<br>JDK安装目录，虽然系统配置了JAVA_HOME，但有时无法正确识别，最后进行配置<br>export JAVA_HOME=/home/app/jdk1.8.0_65<br>指定hadoop的配置文件目录，不运行hadoop可以不指定<br>export HADOOP_CONF_DIR=/home/app/hadoop-2.7.1/etc/hadoop<br>配置jdk安装所在目录<br>配置hadoop配置文件所在目录</p><h3 id="1-4-9-配置core-site-xml"><a href="#1-4-9-配置core-site-xml" class="headerlink" title="1.4.9    配置core-site.xml"></a>1.4.9    配置core-site.xml</h3><pre><code>&lt;configuration&gt;&lt;!--用来指定hdfs的老大，ns为固定属性名，表示两个namenode--&gt;&lt;property&gt;&lt;name&gt;fs.defaultFS&lt;/name&gt;&lt;value&gt;hdfs://ns&lt;/value&gt;&lt;/property&gt;&lt;!--用来指定hadoop运行时产生文件的存放目录--&gt;&lt;property&gt;&lt;name&gt;hadoop.tmp.dir&lt;/name&gt;&lt;value&gt;/home/app/hadoop-2.7.1/tmp&lt;/value&gt;&lt;/property&gt;&lt;!--执行zookeeper地址--&gt;&lt;property&gt;&lt;name&gt;ha.zookeeper.quorum&lt;/name&gt;&lt;value&gt;hadoop1:2181,hadoop2:2181,hadoop3:2181&lt;/value&gt;&lt;/property&gt;    &lt;/configuration&gt;</code></pre><h3 id="1-4-10-配置01节点的hdfs-site-xml"><a href="#1-4-10-配置01节点的hdfs-site-xml" class="headerlink" title="1.4.10    配置01节点的hdfs-site.xml"></a>1.4.10    配置01节点的hdfs-site.xml</h3><p>配置</p><pre><code>&lt;configuration&gt;&lt;!--执行hdfs的nameservice为ns,和core-site.xml保持一致--&gt;&lt;property&gt;&lt;name&gt;dfs.nameservices&lt;/name&gt;&lt;value&gt;ns&lt;/value&gt;&lt;/property&gt;&lt;!--ns下有两个namenode,分别是nn1,nn2--&gt;&lt;property&gt;&lt;name&gt;dfs.ha.namenodes.ns&lt;/name&gt;&lt;value&gt;nn1,nn2&lt;/value&gt;&lt;/property&gt;&lt;!--nn1 的 RPC 通信地址--&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.rpc-address.ns.nn1&lt;/name&gt;&lt;value&gt;hadoop1:9000&lt;/value&gt;&lt;/property&gt;&lt;!--nn1的http通信地址--&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.http-address.ns.nn1&lt;/name&gt;&lt;value&gt;hadoop1:50070&lt;/value&gt;&lt;/property&gt;&lt;!--nn2的RPC通信地址--&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.rpc-address.ns.nn2&lt;/name&gt;&lt;value&gt;hadoop2:9000&lt;/value&gt;&lt;/property&gt;&lt;!--nn2 的 http 通信地址--&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.http-address.ns.nn2&lt;/name&gt;&lt;value&gt;hadoop2:50070&lt;/value&gt;&lt;/property&gt;&lt;!--指定namenode的元数据在JournalNode上的存放位置,这样，namenode2可以 从 jn 集群里获取          最新的namenode的信息，达到热备的效果--&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt;&lt;value&gt;qjournal://hadoop4:8485;hadoop5:8485;hadoop6:8485/ns&lt;/value&gt;&lt;/property&gt;&lt;!--指定 JournalNode 存放数据的位置--&gt;&lt;property&gt;&lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt;&lt;value&gt;/home/app/hadoop-2.7.1/journal&lt;/value&gt;&lt;/property&gt;&lt;!--开启namenode故障时自动切换--&gt;&lt;property&gt;&lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!--配置切换的实现方式--&gt;&lt;property&gt;&lt;name&gt;dfs.client.failover.proxy.provider.ns&lt;/name&gt;&lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt;&lt;/property&gt;&lt;!--配置隔离机制--&gt;&lt;property&gt;&lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt;&lt;value&gt;sshfence&lt;/value&gt;&lt;/property&gt;&lt;!--配置隔离机制的ssh登录秘钥所在的位置--&gt;&lt;property&gt;&lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt;&lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt;&lt;/property&gt;&lt;!--配置namenode数据存放的位置,可以不配置，如果不配置，默认用的是          core-site.xml里配置的hadoop.tmp.dir的路径--&gt;&lt;property&gt;&lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;&lt;value&gt;file:///home/app/hadoop-2.7.1/tmp/namenode&lt;/value&gt;&lt;/property&gt;&lt;!--配置datanode数据存放的位置,可以不配置，如果不配置，默认用的是               core-site.xml 里配置的 hadoop.tmp.dir 的路径--&gt;&lt;property&gt;&lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;&lt;value&gt;file:///home/app/hadoop-2.7.1/tmp/datanode&lt;/value&gt;&lt;/property&gt;&lt;!--配置 block 副本数量--&gt;&lt;property&gt;&lt;name&gt;dfs.replication&lt;/name&gt;&lt;value&gt;3&lt;/value&gt;&lt;/property&gt;&lt;!--设置hdfs的操作权限，false表示任何用户都可以在hdfs上操作文件--&gt;&lt;property&gt;&lt;name&gt;dfs.permissions&lt;/name&gt;&lt;value&gt;false&lt;/value&gt;&lt;/property&gt;    &lt;/configuration&gt;</code></pre><h3 id="1-4-11-配置-mapred-site-xml"><a href="#1-4-11-配置-mapred-site-xml" class="headerlink" title="1.4.11    配置 mapred-site.xml"></a>1.4.11    配置 mapred-site.xml</h3><p>配置代码：</p><pre><code>    &lt;configuration&gt;&lt;property&gt;&lt;!--指定mapreduce运行在yarn上--&gt;&lt;name&gt;mapreduce.framework.name&lt;/name&gt;&lt;value&gt;yarn&lt;/value&gt;&lt;/property&gt;    &lt;/configuration&gt;</code></pre><h3 id="1-4-12-配置yarn-site-xml"><a href="#1-4-12-配置yarn-site-xml" class="headerlink" title="1.4.12    配置yarn-site.xml"></a>1.4.12    配置yarn-site.xml</h3><p>配置代码：</p><pre><code>    &lt;configuration&gt;&lt;!--开启YARN HA --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;  &lt;!--指定两个resourcemanager的名称--&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;&lt;value&gt;rm1,rm2&lt;/value&gt;&lt;/property&gt;&lt;!--配置rm1，rm2的主机--&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;&lt;value&gt;hadoop1&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;&lt;value&gt;hadoop3&lt;/value&gt;&lt;/property&gt;&lt;!--开启yarn恢复机制--&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.recovery.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!--执行rm恢复机制实现类--&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.store.class&lt;/name&gt;&lt;value&gt;org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore&lt;/value&gt;&lt;/property&gt;&lt;!--配置zookeeper的地址--&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;&lt;value&gt;hadoop1:2181,hadoop2:2181,hadoop3:2181&lt;/value&gt;&lt;description&gt;For multiple zk services, separate them with comma&lt;/description&gt;&lt;/property&gt;&lt;!--指定YARN HA的名称--&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;&lt;value&gt;yarn-ha&lt;/value&gt;&lt;/property&gt;&lt;!--指定 yarn 的老大 resoucemanager 的地址--&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;&lt;value&gt;hadoop1&lt;/value&gt;&lt;/property&gt;&lt;!--NodeManager获取数据的方式--&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;    &lt;/configuration&gt;</code></pre><h3 id="1-4-13-配置slaves文件"><a href="#1-4-13-配置slaves文件" class="headerlink" title="1.4.13    配置slaves文件"></a>1.4.13    配置slaves文件</h3><p>配置代码：<br>hadoop4<br>hadoop5<br>hadoop6</p><h3 id="1-4-14-根据配置文件，创建相关的文件夹，用来存放对应数据"><a href="#1-4-14-根据配置文件，创建相关的文件夹，用来存放对应数据" class="headerlink" title="1.4.14    根据配置文件，创建相关的文件夹，用来存放对应数据"></a>1.4.14    根据配置文件，创建相关的文件夹，用来存放对应数据</h3><p>在hadoop-2.7.1目录下创建:<br>①journal目录<br>②创建tmp目录<br>③在tmp目录下，分别创建namenode目录和datanode目录</p><h3 id="1-4-15-配置-hadoop-的环境变量（可不配）"><a href="#1-4-15-配置-hadoop-的环境变量（可不配）" class="headerlink" title="1.4.15    配置 hadoop 的环境变量（可不配）"></a>1.4.15    配置 hadoop 的环境变量（可不配）</h3><p>JAVA_HOME=/home/app/jdk1.8.0_65<br>JAVA_BIN=/home/app/jdk1.8.0_65/bin<br>HADOOP_HOME=/home/app/hadoop-2.7.1<br>PATH=$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH<br>CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar<br>export JAVA_HOME JAVA_BIN HADOOP_HOME PATH CLASSPATH</p><h3 id="1-4-16-通过scp-命令，将hadoop安装目录远程copy到其他5台机器上"><a href="#1-4-16-通过scp-命令，将hadoop安装目录远程copy到其他5台机器上" class="headerlink" title="1.4.16    通过scp 命令，将hadoop安装目录远程copy到其他5台机器上"></a>1.4.16    通过scp 命令，将hadoop安装目录远程copy到其他5台机器上</h3><p>比如向hadoop02节点传输：<br>scp -r hadoop-2.7.1 root@hadoop2:/home/app</p><h3 id="1-4-17-启动zookeeper集群"><a href="#1-4-17-启动zookeeper集群" class="headerlink" title="1.4.17    启动zookeeper集群"></a>1.4.17    启动zookeeper集群</h3><p>在Zookeeper安装目录的bin目录下执行：sh zkServer.sh start<br>sh zkServer.sh status  验证是否启动成功，如成功，两台从节点一台主节点</p><h3 id="1-4-18-格式化zookeeper"><a href="#1-4-18-格式化zookeeper" class="headerlink" title="1.4.18    格式化zookeeper"></a>1.4.18    格式化zookeeper</h3><p>在zk的leader节点上执行：<br>hdfs zkfc -formatZK，这个指令的作用是在zookeeper集群上生成ha节点 （ns节点）<br>注：18–24步可以用一步来替代：进入hadoop安装目录的sbin目录，执行： sh start-dfs.sh 。 但建议还是按部就班来执行，比较可靠。</p><h3 id="1-4-19-启动journalnode集群"><a href="#1-4-19-启动journalnode集群" class="headerlink" title="1.4.19    启动journalnode集群"></a>1.4.19    启动journalnode集群</h3><p>在4、5、6节点上执行：<br>切换到hadoop安装目录的sbin目录下，执行：<br>sh hadoop-daemons.sh start journalnode<br>然后执行jps命令查看进程。</p><h3 id="1-4-20-格式化1节点的namenode"><a href="#1-4-20-格式化1节点的namenode" class="headerlink" title="1.4.20    格式化1节点的namenode"></a>1.4.20    格式化1节点的namenode</h3><p>在1节点上执行：<br>hadoop namenode -format</p><h3 id="1-4-21-启动1节点的namenode"><a href="#1-4-21-启动1节点的namenode" class="headerlink" title="1.4.21    启动1节点的namenode"></a>1.4.21    启动1节点的namenode</h3><p>在 1 节点上执行：<br>sh hadoop-daemon.sh start namenode</p><h3 id="1-4-22-把2节点的namenode节点变为standby-namenode节点"><a href="#1-4-22-把2节点的namenode节点变为standby-namenode节点" class="headerlink" title="1.4.22    把2节点的namenode节点变为standby namenode节点"></a>1.4.22    把2节点的namenode节点变为standby namenode节点</h3><p>在2节点上执行：<br>hdfs namenode -bootstrapStandby</p><h3 id="1-4-23-启动-2-节点的-namenode-节点"><a href="#1-4-23-启动-2-节点的-namenode-节点" class="headerlink" title="1.4.23    启动 2 节点的 namenode 节点"></a>1.4.23    启动 2 节点的 namenode 节点</h3><p>在2节点上执行：<br>sh hadoop-daemon.sh start namenode</p><h3 id="1-4-24-在4-5-6节点上启动datanode节点"><a href="#1-4-24-在4-5-6节点上启动datanode节点" class="headerlink" title="1.4.24    在4,5,6节点上启动datanode节点"></a>1.4.24    在4,5,6节点上启动datanode节点</h3><p>在 4,5,6 节点上执行： sh hadoop-daemon.sh start datanode</p><h3 id="1-4-25-启动zkfc（启动FalioverControllerActive"><a href="#1-4-25-启动zkfc（启动FalioverControllerActive" class="headerlink" title="1.4.25    启动zkfc（启动FalioverControllerActive)"></a>1.4.25    启动zkfc（启动FalioverControllerActive)</h3><p>在1,2节点上执行：<br>sh hadoop-daemon.sh start zkfc</p><h3 id="1-4-26-在1节点上启动主Resourcemanager"><a href="#1-4-26-在1节点上启动主Resourcemanager" class="headerlink" title="1.4.26    在1节点上启动主Resourcemanager"></a>1.4.26    在1节点上启动主Resourcemanager</h3><p>在1节点上执行：start-yarn.sh<br>启动成功后，4,5,6节点上应该有nodemanager 的进程</p><h3 id="1-4-27-在-3-节点上启动副-Resoucemanager"><a href="#1-4-27-在-3-节点上启动副-Resoucemanager" class="headerlink" title="1.4.27    在 3 节点上启动副 Resoucemanager"></a>1.4.27    在 3 节点上启动副 Resoucemanager</h3><p>在3节点上执行：sh yarn-daemon.sh start resourcemanager</p><h2 id="1-5-测试"><a href="#1-5-测试" class="headerlink" title="1.5    测试"></a>1.5    测试</h2><p>输入地址： <a href="http://192.168.65.121:50070" target="_blank" rel="noopener">http://192.168.65.121:50070</a> ，查看 namenode 的信息，是active状态<br><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/pictures/5.png" alt="5"></p><p>输入地址：<a href="http://192.168.65.122:50070查看namenode的信息，是standby状态" target="_blank" rel="noopener">http://192.168.65.122:50070查看namenode的信息，是standby状态</a><br><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/pictures/6.png" alt="6"></p><p>然后停掉01节点的namenode,此时发现standby的namenode变为active。<br>如果出现standby节点不能正确替代active的情况检查fuser是否正常<br>yum provides “*/fuser”</p><p>在启动namenode的两台节点上安装fuser,分别执行<br>yum -y install psmisc</p><p><a href="http://f.dataguru.cn/hadoop-707122-1-1.html" target="_blank" rel="noopener">http://f.dataguru.cn/hadoop-707122-1-1.html</a></p><hr><p><strong>持续更新中…，如果遇到问题欢迎联系我，在文章最后评论区【留言和讨论】，当然，欢迎点击文章最后的打赏按键，请博主一杯冰阔乐，笑～</strong></p><table>  <tbody><tr>    <td><img width="100" src="https://hualei.online/medias/reward/alipay.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/wechat.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/zan.png"></td>     </tr></tbody></table><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 集群搭建 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Hadoop </tag>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一招搞定GitHub下载加速!</title>
      <link href="/posts/fdc0.html"/>
      <url>/posts/fdc0.html</url>
      
        <content type="html"><![CDATA[<h1 id="一个痛点"><a href="#一个痛点" class="headerlink" title="一个痛点"></a>一个痛点</h1><p>众所周知，GitHub是一个巨大的开源宝库，以及程序员和编程爱好者的聚集地，包括我之前推荐的诸多优秀的开源项目全部都是位于GitHub上。但是每当我们看到优秀的开源项目，准备去下（bai）载（piao）时，会发现 git clone的速度异常之慢！就我个人而言，在我家里200M移动宽带的环境下，我克隆开源项目就没发现速度大于过 20.00KiB/s的时候，这简直太难受了。</p><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/pictures/%E5%9B%BE%E7%89%87%E4%B8%80.jpg" alt="图片一"><br>小项目倒还好，我等几分钟无所谓；一旦项目庞大起来，或者项目文件数目一多， git clone 大概率会失败！<br>当然网上常见的诸如修改hosts、代理等方式实际使用效果并不一定好，而且也不稳定。</p><h1 id="码云”是个好东西"><a href="#码云”是个好东西" class="headerlink" title="码云”是个好东西"></a>码云”是个好东西</h1><p>接下来就介绍一种GitHub下载的加速方法：通过国内码云平台的转接，来完成GitHub上项目的下载加速。<br> （1）首先确保码云上有账户，可以正常使用，没有的可以自行注册一下。<br> （2）点击右上角新建仓库的加号 +，选择“从 GitHub/GitLab导入仓库”菜单<br><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/pictures/%E5%9B%BE%E7%89%87%E4%BA%8C.jpg" alt="图片二"><br>  (3)然后填写位于 GitHub上你想 clone的仓库地址并导入<br><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/pictures/%E5%9B%BE%E7%89%87%E4%B8%89.jpg" alt="图片三"><br>这一步交给码云来做速度是非常快的，一会儿功夫，码云就克隆出了一份和GitHub上一模一样的项目！<br><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/pictures/%E5%9B%BE%E7%89%87%E5%9B%9B.jpg" alt="图片四"><br>  (4）接下来我们通过码云上的项目地址，将项目 clone到本地，这时候的clone速度就很快了，几 MB/s的速度是没问题的，很快项目就下载下来了。按道理讲这时候我们的目的已经达到了，不过不要忘了，还有一件事没做。</p><hr><h1 id="重新关联远端地址"><a href="#重新关联远端地址" class="headerlink" title="重新关联远端地址"></a>重新关联远端地址</h1><p>要知道，这时候克隆到本地的项目关联的是码云Gitee的地址，已经和原来的GitHub项目完全脱离了，是另外一个副本。<br>在必要情况下（比如我们就是要给GitHub上的某个项目提 PR），我们还需要重新将我们本地的项目关联到原来的GitHub项目上去，做法如下：<br> （1）首先找到位于本地仓库目录下的隐藏文件夹 .git<br><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/pictures/%E5%9B%BE%E7%89%87%E4%BA%94.jpg" alt="图片五"><br> （2）用文本编辑器打开 .git文件夹中的 config配置文件<br><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/pictures/%E5%9B%BE%E7%89%87%E5%85%AD.jpg" alt="图片六"><br>  (3)将配置文件中的 [remote “origin”].url字段重新关联到原来位于GitHub上的GitHub项目地址<br><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/pictures/%E5%9B%BE%E7%89%87%E4%B8%83.jpg" alt="图片七"><br>当然你也可以通过命令行来修改远端地址，效果一样的<br>至此大功告成，本地项目就相当于是 clone自GitHub，后续提代码，提 PR到GitHub上都没有问题。</p><hr><p><strong>持续更新中…，如果遇到问题欢迎联系我，在文章最后评论区【留言和讨论】，当然，欢迎点击文章最后的打赏按键，请博主一杯冰阔乐，笑～</strong></p><table>  <tbody><tr>    <td><img width="100" src="https://hualei.online/medias/reward/alipay.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/wechat.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/zan.png"></td>     </tr></tbody></table><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 软件使用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
            <tag> 软件 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL分库分表方案汇总</title>
      <link href="/posts/fd29.html"/>
      <url>/posts/fd29.html</url>
      
        <content type="html"><![CDATA[<h1 id="一、数据库瓶颈"><a href="#一、数据库瓶颈" class="headerlink" title="一、数据库瓶颈"></a>一、数据库瓶颈</h1><p>不管是IO瓶颈，还是CPU瓶颈，最终都会导致数据库的活跃连接数增加，进而逼近甚至达到数据库可承载活跃连接数的阈值。在业务Service来看就是，可用数据库连接少甚至无连接可用。接下来就可以想象了吧（并发量、吞吐量、崩溃）。</p><h2 id="1、IO瓶颈"><a href="#1、IO瓶颈" class="headerlink" title="1、IO瓶颈"></a>1、IO瓶颈</h2><p>第一种：磁盘读IO瓶颈，热点数据太多，数据库缓存放不下，每次查询时会产生大量的IO，降低查询速度 -&gt; 分库和垂直分表。<br>第二种：网络IO瓶颈，请求的数据太多，网络带宽不够 -&gt; 分库。</p><h2 id="2、CPU瓶颈"><a href="#2、CPU瓶颈" class="headerlink" title="2、CPU瓶颈"></a>2、CPU瓶颈</h2><p>第一种：SQL问题，如SQL中包含join，group by，order by，非索引字段条件查询等，增加CPU运算的操作 -&gt; SQL优化，建立合适的索引，在业务Service层进行业务计算。<br>第二种：单表数据量太大，查询时扫描的行太多，SQL效率低，CPU率先出现瓶颈 -&gt; 水平分表。</p><hr><h1 id="二、分库分表"><a href="#二、分库分表" class="headerlink" title="二、分库分表"></a>二、分库分表</h1><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/pictures/%E6%95%B0%E6%8D%AE%E5%BA%931.jpg" alt="数据库1"></p><h2 id="1、水平分库"><a href="#1、水平分库" class="headerlink" title="1、水平分库"></a>1、水平分库</h2><p>概念：以字段为依据，按照一定策略（hash、range等），将一个库中的数据拆分到多个库中。</p><p>结果：<br>每个库的结构都一样；<br>每个库的数据都不一样，没有交集；<br>所有库的并集是全量数据；</p><p>场景：系统绝对并发量上来了，分表难以根本上解决问题，并且还没有明显的业务归属来垂直分库。<br>分析：库多了，io和cpu的压力自然可以成倍缓解。</p><h2 id="2、水平分表"><a href="#2、水平分表" class="headerlink" title="2、水平分表"></a>2、水平分表</h2><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/pictures/%E6%95%B0%E6%8D%AE%E5%BA%932.jpg" alt="数据库2"><br>概念：以字段为依据，按照一定策略（hash、range等），将一个表中的数据拆分到多个表中。</p><p>结果：<br>每个表的结构都一样；<br>每个表的数据都不一样，没有交集；<br>所有表的并集是全量数据；<br>场景：系统绝对并发量并没有上来，只是单表的数据量太多，影响了SQL效率，加重了CPU负担，以至于成为瓶颈。推荐：一次SQL查询优化原理分析<br>分析：表的数据量少了，单次SQL执行效率高，自然减轻了CPU的负担。</p><h2 id="3、垂直分库"><a href="#3、垂直分库" class="headerlink" title="3、垂直分库"></a>3、垂直分库</h2><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/pictures/%E6%95%B0%E6%8D%AE%E5%BA%933.jpg" alt="数据库3"><br>概念：以表为依据，按照业务归属不同，将不同的表拆分到不同的库中。</p><p>结果：<br>每个库的结构都不一样；<br>每个库的数据也不一样，没有交集；<br>所有库的并集是全量数据；<br>场景：系统绝对并发量上来了，并且可以抽象出单独的业务模块。<br>分析：到这一步，基本上就可以服务化了。例如，随着业务的发展一些公用的配置表、字典表等越来越多，这时可以将这些表拆到单独的库中，甚至可以服务化。再有，随着业务的发展孵化出了一套业务模式，这时可以将相关的表拆到单独的库中，甚至可以服务化。</p><h2 id="4、垂直分表"><a href="#4、垂直分表" class="headerlink" title="4、垂直分表"></a>4、垂直分表</h2><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://hualei.online/medias/pictures/%E6%95%B0%E6%8D%AE%E5%BA%934.jpg" alt="数据库4"><br>概念：以字段为依据，按照字段的活跃性，将表中字段拆到不同的表（主表和扩展表）中。</p><p>结果：<br>每个表的结构都不一样；<br>每个表的数据也不一样，一般来说，每个表的字段至少有一列交集，一般是主键，用于关联数据；<br>所有表的并集是全量数据；<br>场景：系统绝对并发量并没有上来，表的记录并不多，但是字段多，并且热点数据和非热点数据在一起，单行数据所需的存储空间较大。以至于数据库缓存的数据行减少，查询时会去读磁盘数据产生大量的随机读IO，产生IO瓶颈。<br>分析：可以用列表页和详情页来帮助理解。垂直分表的拆分原则是将热点数据（可能会冗余经常一起查询的数据）放在一起作为主表，非热点数据放在一起作为扩展表。这样更多的热点数据就能被缓存下来，进而减少了随机读IO。拆了之后，要想获得全部数据就需要关联两个表来取数据。<br>但记住，千万别用join，因为join不仅会增加CPU负担并且会讲两个表耦合在一起（必须在一个数据库实例上）。关联数据，应该在业务Service层做文章，分别获取主表和扩展表数据然后用关联字段关联得到全部数据。</p><hr><h1 id="三、分库分表工具"><a href="#三、分库分表工具" class="headerlink" title="三、分库分表工具"></a>三、分库分表工具</h1><p>1.sharding-sphere：jar，前身是sharding-jdbc；<br>2.TDDL：jar，Taobao Distribute Data Layer；<br>3.Mycat：中间件。<br>    注：工具的利弊，请自行调研，官网和社区优先。</p><h1 id="四、分库分表步骤"><a href="#四、分库分表步骤" class="headerlink" title="四、分库分表步骤"></a>四、分库分表步骤</h1><p>根据容量（当前容量和增长量）评估分库或分表个数 -&gt; 选key（均匀）-&gt; 分表规则（hash或range等）-&gt; 执行（一般双写）-&gt; 扩容问题（尽量减少数据的移动）。<br>扩展：MySQL：分库分表与分区的区别和思考</p><h1 id="五、分库分表总结"><a href="#五、分库分表总结" class="headerlink" title="五、分库分表总结"></a>五、分库分表总结</h1><p>分库分表，首先得知道瓶颈在哪里，然后才能合理地拆分（分库还是分表？水平还是垂直？分几个？）。且不可为了分库分表而拆分。<br>选key很重要，既要考虑到拆分均匀，也要考虑到非partition key的查询。<br>只要能满足需求，拆分规则越简单越好。</p><hr><p><strong>持续更新中…，如果遇到问题欢迎联系我，在文章最后评论区【留言和讨论】，当然，欢迎点击文章最后的打赏按键，请博主一杯冰阔乐，笑～</strong></p><table>  <tbody><tr>    <td><img width="100" src="https://hualei.online/medias/reward/alipay.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/wechat.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/zan.png"></td>     </tr></tbody></table><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 数据库 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
            <tag> IT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpringBoot框架的使用</title>
      <link href="/posts/7c5b.html"/>
      <url>/posts/7c5b.html</url>
      
        <content type="html"><![CDATA[<h1 id="简短介绍"><a href="#简短介绍" class="headerlink" title="简短介绍"></a>简短介绍</h1><p>概述<br>随着动态语言的流行（Ruby、Groovy、Scala、Node.js），Java的开发显得格外的笨重：繁多的配置、低下的开发效率、复杂的部署流程以及第三方技术集成难度大。<br>在上述环境下，Springboot应运而生。它使用”习惯优于配置”（项目中存在大量的配置，此外还内置一个习惯性的配置，让你无须手动进行配置）的理念让你的项目快速运行起来。使用springboot很容易创建一个独立运行（运行jar，内嵌servlet容器）、准生产级别的基于Spring框架的项目，使用springboot你可以不用或者只需要很少的Spring配置。</p><hr><h1 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h1><p>SpringBoot 入门实例<br>Spring Boot 的核心功能<br>独立运行的Spring项目<br>Spring Boot可以以jar包的形式独立运行，运行一个Spring Boot项目只需要通过java -jar xx.jar。<br>内置Servlet容器<br>Spring Boot可选择内嵌Tomcat、Jetty或者Undertow，这样无须以war包形式部署。<br>提供starter简化maven配置<br>Spring提供了一系列的starter pom来简化maven依赖加载，例如：当你使用了spring-boot-starter-web时，会自动加入相关依赖，无需你手动一个一个的添加坐标依赖。<br>自动配置Spring<br>Spring Boot会根据在类路径中的jar包、类，为jar包里的类自动配置Bean，这样会极大地减少我们要使用的配置。当然，Spring Boot只是考虑了大多数的开发场景，并不是所有场景，若在实际开发中，我们需要自动配置bean，而Spring Boot没有提供支持，则可以自定义自动配置。<br>无代码生成和xml配置<br>Spring Boot的神奇的不是借助于代码生成来实现的，而是通过条件注解来实现的，这是Spring 4.x提供的新特性，Spring 4.x提倡使用java配置和注解配置相结合，而Spring Boot不需要任何xml配置即可实现Sping Boot的所有配置。<br>优缺点</p><p>优点<br>快速构建项目：省略了繁琐且重复的xml配置，分分钟构建一个web工程；<br>对主流开发框架的无配置集成：提供了很多Starter 依赖包，开箱即用，无需多余配置；<br>项目可独立运行：无需外部依赖Servlet容器；<br>极大地提供了开发、部署效率；<br>监控简单：提供了actuator包，可以使用它来对你的应用进行监控。</p><p>缺点<br>依赖太多：一个简单的SpringBoot应用都有好几十M只有；<br>缺少监控集成方案、安全管理方案：只提供基础监控，要实现生产级别的监控，监控方案需要自己动手解决；(后期讲解soringCloud时，会结合pinpoint和skywalking分布式链路工具进行应用监控)</p><hr><h1 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h1><p>正文<br>在 eclips e中创建 maven project，创建过程中不使用 archetype，packaging 属性默认 jar 即可。<br>pom.xml 中添加 SpringBoot 依赖<br><parent><br>    <groupid>org.springframework.boot</groupid><br>    <artifactid>spring-boot-starter-parent</artifactid><br>    <version>2.0.2.RELEASE</version><br></parent></p><dependencies>    <dependency>        <groupid>org.springframework.boot</groupid>        <artifactid>spring-boot-starter-web</artifactid>    </dependency></dependencies>在 src/main/java 下创建基础包，本文为 com.aotian.demo，然后创建一个启动类，本文创建为 SpringBootDemo，这个类用于启动 SpringBoot 内置容器，可以理解为 SpringBoot 程序的入口，SpringBoot 会扫描这个类所在包的所有子包中的内容。创建后添加 main 方法和注解,代码如下：package com.aotian.demo;<p>import org.springframework.boot.SpringApplication;<br>import org.springframework.boot.autoconfigure.EnableAutoConfiguration;</p><p>/**</p><ul><li><p>springboot启动程序</p></li><li><p>@author aotian</p></li><li></li><li><p>/<br>@SpringBootApplication<br>public class SpringBootDemo {</p><p>  public static void main(String[] args) {</p><pre><code>  SpringApplication.run(SpringBootDemo.class, args);</code></pre><p>  }</p></li></ul><p>}<br>在上一步的基础包下继续创建子包，本文创建的是 controller 包，然后创建具体的业务类，本文创建为 DemoController，其中代码与 SpringMVC 写法一样，代码如下：<br>package com.aotian.demo.controller;</p><p>import org.springframework.web.bind.annotation.RequestMapping;<br>import org.springframework.web.bind.annotation.RestController;</p><p>/**</p><ul><li><p>演示项目控制类</p></li><li><p>@author aotian</p></li><li></li><li><p>/<br>@RestController<br>public class DemoController {</p><p>  @RequestMapping(“/helloSpringBoot”)<br>  public String helloSpringBoot() {</p><pre><code>  return "Hello Spring Boot";</code></pre><p>  }</p></li></ul><p>}<br>启动 SpringBoot 程序。返回之前的 SpringBootDemo 类中运行 main 方法，可在控制台看到如下内容，其中红线标注的部分提示到：<br>springmvc 默认访问地址为 /<br>tomcat 默认端口为 8080<br>springboot 程序启动成功</p><p>spring boot 启动页面<br>在浏览器中输入 <a href="http://localhost:8080/helloSpringBoot" target="_blank" rel="noopener">http://localhost:8080/helloSpringBoot</a> 即可在浏览器中查看其返回的字符串。</p><p>到此，一个简单的Spring Boot程序就开发完了，与Spring MVC 的开发效率比较起来，那一定是一个跨时代的飞跃，减少了烦人的XML配置文件，这样程序看起来就会轻松很多。</p><p><strong>持续更新中…，如果遇到问题欢迎联系我，在文章最后评论区【留言和讨论】，当然，欢迎点击文章最后的打赏按键，请博主一杯冰阔乐，笑～</strong></p><table>  <tbody><tr>    <td><img width="100" src="https://hualei.online/medias/reward/alipay.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/wechat.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/zan.png"></td>     </tr></tbody></table><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 技术的学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 博客 </tag>
            
            <tag> 技术 </tag>
            
            <tag> IT </tag>
            
            <tag> 程序 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>博客开源</title>
      <link href="/posts/be38.html"/>
      <url>/posts/be38.html</url>
      
        <content type="html"><![CDATA[<h1 id="效果演示"><a href="#效果演示" class="headerlink" title="效果演示"></a>效果演示</h1><p><img src="https://cdn.jsdelivr.net/gh/shw2018/cdn@1.0/sakura/img/loader/orange.progress-bar-stripe-loader.svg" data-original="https://raw.githubusercontent.com/shw2018/cdn/master/blog_files/img/Blog-Open-Source/blog-demo1.gif" alt="演示Demo"></p><h1 id="简短介绍"><a href="#简短介绍" class="headerlink" title="简短介绍"></a>简短介绍</h1><p>倒腾了一两周总算把个人博客网站完善了，目前这个版本使用应该是够了，当然还有一些优化项和功能增加后续在慢慢更新.</p><p>本博客基于<code>Hexo</code>框架搭建，用到<a href="https://github.com/hualei1234567" target="_blank" rel="noopener">hexo-theme-matery</a>主题, 并在此基础之上做了很多修改，修复了一些bug，增加了一些新的特性和功能，博客地址：<a href="https://hualei1234567.github.io/" target="_blank" rel="noopener">https://hualei1234567.github.io</a>，博客演示：<a href="https://hualei.online">hualei.online</a>。</p><hr><h2 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h2><p><strong>原主题特性</strong>:</p><ul><li><p>简单漂亮，文章内容美观易读</p></li><li><p><a href="https://material.io/" target="_blank" rel="noopener">Material Design</a> 设计</p></li><li><p>响应式设计，博客在桌面端、平板、手机等设备上均能很好的展现</p></li><li><p>首页轮播文章及每天动态切换 <code>Banner</code> 图片</p></li><li><p>瀑布流式的博客文章列表（文章无特色图片时会有 <code>24</code> 张漂亮的图片代替）</p></li><li><p>时间轴式的归档页</p></li><li><p><strong>词云</strong>的标签页和<strong>雷达图</strong>的分类页</p></li><li><p>丰富的关于我页面（包括关于我、文章统计图、我的项目、我的技能、相册等）</p></li><li><p>可自定义的数据的友情链接页面</p></li><li><p>支持文章置顶和文章打赏</p></li><li><p>支持 <code>MathJax</code></p></li><li><p><code>TOC</code> 目录</p></li><li><p>可设置复制文章内容时追加版权信息</p></li><li><p>可设置阅读文章时做密码验证</p></li><li><p><a href="https://gitalk.github.io/" target="_blank" rel="noopener">Gitalk</a>、<a href="https://imsun.github.io/gitment/" target="_blank" rel="noopener">Gitment</a>、<a href="https://valine.js.org/" target="_blank" rel="noopener">Valine</a> 和 <a href="https://disqus.com/" target="_blank" rel="noopener">Disqus</a> 评论模块（推荐使用 <code>Gitalk</code>）</p></li><li><p>集成了<a href="http://busuanzi.ibruce.info/" target="_blank" rel="noopener">不蒜子统计</a>、谷歌分析（<code>Google Analytics</code>）和文章字数统计等功能</p></li><li><p>支持在首页的音乐播放和视频播放功能</p><p><strong>增加的工作或特性(未打钩的是已做但还没更新到源码的)</strong>:</p></li><li><p>修改了原主题的一些很多<code>bug</code>   2019.08.05</p></li><li><p>加入图片懒加载功能，在根目录配置文件开启和关闭    2019.08.09</p></li><li><p>增加<code>留言板</code>功能          2019.08.05</p></li><li><p>在关于板块,加入<code>简历</code>功能页   2019.08.05</p></li><li><p>增加视听[视觉听觉影音]板块       2019.08.10</p></li><li><p>支持<code>emoji</code>表情，用<code>markdown emoji</code>语法书写直接生成对应的能<strong>跳跃</strong>的表情。  2019.08.10</p></li><li><p>增加网站运行时间显示  2019.08.10</p></li><li><p>增加<code>动漫模型</code>     2019.08.10</p></li><li><p>整体替换Banner图片和文章特色图片   2019.08.10</p></li><li><p>增加分类<code>相册</code>功能         2019.08.29</p></li><li><p>去掉标签页,将其合并至<code>分类</code>页中                2019.09.01</p></li><li><p>修改了一些控件的参数   2019.09.01</p></li><li><p>修改部分样式,比如: 文章卡片,固定高度,使其不至于因为文章摘要的长短不同导致卡片大小不一使页面布局很不美观,类似的还有友链卡片,优化了页面内容布局,视觉更整齐美观          2019.09.01</p></li><li><p>解决首页文章列表卡片上方 <code>border-radius</code>圆角失效的bug  2019.09.01</p></li><li><p>添加页面樱花飘落动效            2019.09.09</p></li><li><p>添加鼠标点击烟花爆炸动效   2019.09.09</p></li><li><p>加入天气接口控件   2019.09.09</p></li><li><p>加入鼠标点击文字特效   2019.09.10</p></li><li><p>添加页面雪花飘落动效            2019.09.10</p></li><li><p>添加在线聊天插件            2019.09.12</p></li><li><p>持续更新…</p></li></ul><hr><blockquote><p><strong>更多详情教程，强烈推荐大佬写的：<a href="https://sunhwee.com/posts/6e8839eb.html" target="_blank" rel="noopener">Hexo+Github博客搭建完全教程</a></strong></p></blockquote><blockquote><p><strong>最后，如果项目和教程对你有所帮助或者你看见了还算比较喜欢，欢迎给我<code>star</code>，谢谢您！</strong></p></blockquote><p><strong>持续更新中…，如果遇到问题欢迎联系我，在文章最后评论区【留言和讨论】，当然，欢迎点击文章最后的打赏按键，请博主一杯冰阔乐，笑～</strong></p><table>  <tbody><tr>    <td><img width="100" src="https://hualei.online/medias/reward/alipay.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/wechat.bmp"></td>    <td><img width="100" src="https://hualei.online/medias/reward/zan.png"></td>     </tr></tbody></table><script>        document.querySelectorAll('.github-emoji')          .forEach(el => {            if (!el.dataset.src) { return; }            const img = document.createElement('img');            img.style = 'display:none !important;';            img.src = el.dataset.src;            img.addEventListener('error', () => {              img.remove();              el.style.color = 'inherit';              el.style.backgroundImage = 'none';              el.style.background = 'none';            });            img.addEventListener('load', () => {              img.remove();            });            document.body.appendChild(img);          });      </script>]]></content>
      
      
      <categories>
          
          <category> 软件安装与配置 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hexo </tag>
            
            <tag> Github </tag>
            
            <tag> 博客 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
